{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xldQkYQiCYm",
        "outputId": "079684ac-c23e-4ef1-e5eb-819bb0aa6789"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (3.4.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly) (9.0.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas numpy matplotlib seaborn plotly networkx scikit-learn tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import uuid\n",
        "import os\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import logging\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger('marketing_attribution')\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def debug_unified_data(unified_data):\n",
        "    \"\"\"Print diagnostic information about the unified data.\"\"\"\n",
        "    print(\"\\n--- UNIFIED DATA DIAGNOSTICS ---\")\n",
        "    print(f\"Data shape: {unified_data.shape}\")\n",
        "    print(f\"Data types:\\n{unified_data.dtypes}\")\n",
        "    print(\"\\nSample data (first 3 rows):\")\n",
        "    print(unified_data.head(3))\n",
        "\n",
        "    # Check key columns\n",
        "    for col in ['customer_id', 'timestamp', 'data_source', 'is_conversion']:\n",
        "        if col not in unified_data.columns:\n",
        "            print(f\"ERROR: Required column '{col}' is missing!\")\n",
        "        else:\n",
        "            print(f\"\\nColumn '{col}' stats:\")\n",
        "            if col == 'timestamp':\n",
        "                if pd.api.types.is_datetime64_any_dtype(unified_data[col]):\n",
        "                    print(f\"  Type: datetime (good!)\")\n",
        "                    print(f\"  Min: {unified_data[col].min()}\")\n",
        "                    print(f\"  Max: {unified_data[col].max()}\")\n",
        "                else:\n",
        "                    print(f\"  Type: {unified_data[col].dtype} (NEEDS CONVERSION TO DATETIME!)\")\n",
        "                    print(f\"  Sample values: {unified_data[col].head(3).tolist()}\")\n",
        "            elif col == 'is_conversion':\n",
        "                print(f\"  Type: {unified_data[col].dtype}\")\n",
        "                print(f\"  Unique values: {unified_data[col].unique().tolist()}\")\n",
        "                if not pd.api.types.is_bool_dtype(unified_data[col]):\n",
        "                    print(\"  WARNING: 'is_conversion' should be boolean (True/False)\")\n",
        "            else:\n",
        "                print(f\"  Type: {unified_data[col].dtype}\")\n",
        "                print(f\"  Sample values: {unified_data[col].head(3).tolist()}\")\n",
        "                print(f\"  Unique count: {unified_data[col].nunique()}\")\n",
        "\n",
        "    # Check for NaN values\n",
        "    na_counts = unified_data.isna().sum()\n",
        "    print(\"\\nNaN counts:\")\n",
        "    print(na_counts[na_counts > 0] if any(na_counts > 0) else \"No NaN values\")"
      ],
      "metadata": {
        "id": "56eImfv4iJq5"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_optimized_data_files():\n",
        "    \"\"\"Create data files optimized for the Marketing Attribution system.\"\"\"\n",
        "    print(\"Generating optimized data files...\")\n",
        "\n",
        "    # Create directory\n",
        "    output_dir = 'simulated_data'\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Generate sample data\n",
        "    num_customers = 500\n",
        "    num_days = 90\n",
        "    start_date = datetime.datetime(2023, 1, 1)\n",
        "    end_date = start_date + datetime.timedelta(days=num_days)\n",
        "\n",
        "    # Customer IDs\n",
        "    customer_ids = [f\"cust_{i:04d}\" for i in range(num_customers)]\n",
        "\n",
        "    # Create unified data directly\n",
        "    unified_data = []\n",
        "\n",
        "    # Generate touchpoints across multiple channels\n",
        "    channels = ['website_analytics', 'email_campaigns', 'paid_ads']\n",
        "\n",
        "    for customer_id in customer_ids:\n",
        "        # Number of touchpoints (1-10)\n",
        "        num_touchpoints = random.randint(1, 10)\n",
        "\n",
        "        # Decide if this customer will convert (15% chance)\n",
        "        will_convert = random.random() < 0.15\n",
        "\n",
        "        # Generate touchpoints\n",
        "        for i in range(num_touchpoints):\n",
        "            # Pick a channel\n",
        "            channel = random.choice(channels)\n",
        "\n",
        "            # Generate timestamp\n",
        "            timestamp = start_date + datetime.timedelta(\n",
        "                days=random.randint(0, num_days),\n",
        "                hours=random.randint(0, 23),\n",
        "                minutes=random.randint(0, 59)\n",
        "            )\n",
        "\n",
        "            # Base touchpoint data\n",
        "            touchpoint = {\n",
        "                'customer_id': customer_id,\n",
        "                'timestamp': timestamp,\n",
        "                'data_source': channel,\n",
        "                'is_conversion': False,  # Default to False\n",
        "                'cost': random.uniform(0.1, 2.0)\n",
        "            }\n",
        "\n",
        "            # Add channel-specific fields\n",
        "            if channel == 'website_analytics':\n",
        "                touchpoint.update({\n",
        "                    'page_path': random.choice(['home', 'product', 'blog', 'about']),\n",
        "                    'session_id': f\"session_{uuid.uuid4().hex[:8]}\"\n",
        "                })\n",
        "            elif channel == 'email_campaigns':\n",
        "                touchpoint.update({\n",
        "                    'campaign_name': random.choice(['newsletter', 'promotion', 'welcome']),\n",
        "                    'subject': random.choice(['Special Offer', 'New Products', 'Welcome'])\n",
        "                })\n",
        "            elif channel == 'paid_ads':\n",
        "                touchpoint.update({\n",
        "                    'platform': random.choice(['google_ads', 'facebook_ads']),\n",
        "                    'campaign': random.choice(['brand', 'remarketing', 'acquisition'])\n",
        "                })\n",
        "\n",
        "            unified_data.append(touchpoint)\n",
        "\n",
        "        # Add conversion touchpoint if this customer converts\n",
        "        if will_convert:\n",
        "            # Conversion is always the last touchpoint\n",
        "            conv_timestamp = start_date + datetime.timedelta(\n",
        "                days=random.randint(0, num_days),\n",
        "                hours=random.randint(0, 23),\n",
        "                minutes=random.randint(0, 59)\n",
        "            )\n",
        "\n",
        "            # Ensure conversion timestamp is after all other touchpoints\n",
        "            existing_timestamps = [tp['timestamp'] for tp in unified_data\n",
        "                                 if tp['customer_id'] == customer_id]\n",
        "            if existing_timestamps:\n",
        "                conv_timestamp = max(existing_timestamps) + datetime.timedelta(\n",
        "                    minutes=random.randint(1, 60))\n",
        "\n",
        "            conversion_touchpoint = {\n",
        "                'customer_id': customer_id,\n",
        "                'timestamp': conv_timestamp,\n",
        "                'data_source': 'paid_ads',  # Conversions typically come from ads\n",
        "                'is_conversion': True,\n",
        "                'platform': random.choice(['google_ads', 'facebook_ads']),\n",
        "                'campaign': random.choice(['brand', 'remarketing', 'acquisition']),\n",
        "                'cost': random.uniform(0.5, 2.0)\n",
        "            }\n",
        "\n",
        "            unified_data.append(conversion_touchpoint)\n",
        "\n",
        "    # Create DataFrame\n",
        "    unified_df = pd.DataFrame(unified_data)\n",
        "\n",
        "    # Sort by customer and timestamp\n",
        "    unified_df = unified_df.sort_values(['customer_id', 'timestamp'])\n",
        "\n",
        "    # Save unified data\n",
        "    unified_df.to_csv(os.path.join(output_dir, 'unified_data.csv'), index=False)\n",
        "    print(f\"Saved unified_data to {output_dir}/unified_data.csv\")\n",
        "\n",
        "    # Also create individual channel files\n",
        "    for channel in channels:\n",
        "        channel_df = unified_df[unified_df['data_source'] == channel].copy()\n",
        "\n",
        "        if channel == 'website_analytics':\n",
        "            channel_df.rename(columns={'customer_id': 'user_id'}, inplace=True)\n",
        "            channel_df = channel_df[['user_id', 'timestamp', 'page_path', 'session_id', 'cost']]\n",
        "        elif channel == 'email_campaigns':\n",
        "            channel_df.rename(columns={'customer_id': 'email_id', 'timestamp': 'date'}, inplace=True)\n",
        "            channel_df = channel_df[['email_id', 'date', 'campaign_name', 'subject', 'cost']]\n",
        "        elif channel == 'paid_ads':\n",
        "            channel_df.rename(columns={'customer_id': 'click_id', 'timestamp': 'click_time'}, inplace=True)\n",
        "            channel_df['purchase'] = channel_df['is_conversion'].astype(int)\n",
        "            channel_df = channel_df[['click_id', 'click_time', 'platform', 'campaign', 'cost', 'purchase']]\n",
        "\n",
        "        # Save to CSV\n",
        "        channel_df.to_csv(os.path.join(output_dir, f'{channel}.csv'), index=False)\n",
        "        print(f\"Saved {channel} to {output_dir}/{channel}.csv\")\n",
        "\n",
        "    return unified_df"
      ],
      "metadata": {
        "id": "sqyU2RdOiOHb"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def manual_customer_journey_creation(unified_data):\n",
        "    \"\"\"Manually create customer journeys from unified data.\"\"\"\n",
        "    print(\"\\nManually creating customer journeys...\")\n",
        "\n",
        "    if unified_data is None or len(unified_data) == 0:\n",
        "        print(\"ERROR: No unified data available\")\n",
        "        return None\n",
        "\n",
        "    # Ensure required columns exist\n",
        "    required_cols = ['customer_id', 'timestamp', 'data_source', 'is_conversion']\n",
        "    missing_cols = [col for col in required_cols if col not in unified_data.columns]\n",
        "    if missing_cols:\n",
        "        print(f\"ERROR: Missing required columns: {missing_cols}\")\n",
        "        return None\n",
        "\n",
        "    # Ensure timestamp is datetime\n",
        "    if not pd.api.types.is_datetime64_any_dtype(unified_data['timestamp']):\n",
        "        print(\"Converting timestamp to datetime...\")\n",
        "        unified_data['timestamp'] = pd.to_datetime(unified_data['timestamp'])\n",
        "\n",
        "    # Ensure is_conversion is boolean\n",
        "    if not pd.api.types.is_bool_dtype(unified_data['is_conversion']):\n",
        "        print(\"Converting is_conversion to boolean...\")\n",
        "        unified_data['is_conversion'] = unified_data['is_conversion'].astype(bool)\n",
        "\n",
        "    # Sort by customer and timestamp\n",
        "    unified_data = unified_data.sort_values(['customer_id', 'timestamp'])\n",
        "\n",
        "    # Create journeys dictionary\n",
        "    journeys = {}\n",
        "    touchpoint_sequences = []\n",
        "\n",
        "    # Group by customer_id\n",
        "    for customer_id, group in unified_data.groupby('customer_id'):\n",
        "        touchpoints = group['data_source'].tolist()\n",
        "        timestamps = group['timestamp'].tolist()\n",
        "        converted = any(group['is_conversion'])\n",
        "\n",
        "        # Create journey\n",
        "        journey = {\n",
        "            'customer_id': customer_id,\n",
        "            'touchpoints': touchpoints,\n",
        "            'timestamps': timestamps,\n",
        "            'converted': converted\n",
        "        }\n",
        "\n",
        "        journeys[customer_id] = journey\n",
        "\n",
        "        # Record touchpoint sequence\n",
        "        sequence = {\n",
        "            'customer_id': customer_id,\n",
        "            'sequence': '->'.join(touchpoints),\n",
        "            'touchpoint_count': len(touchpoints),\n",
        "            'converted': converted\n",
        "        }\n",
        "        touchpoint_sequences.append(sequence)\n",
        "\n",
        "    print(f\"Manually created {len(journeys)} customer journeys\")\n",
        "\n",
        "    # Sample journeys for inspection\n",
        "    sample_journeys = list(journeys.values())[:5]\n",
        "    print(\"\\nSample journeys (first 5):\")\n",
        "    for i, journey in enumerate(sample_journeys):\n",
        "        print(f\"\\nJourney {i+1}:\")\n",
        "        print(f\"  Customer: {journey['customer_id']}\")\n",
        "        print(f\"  Touchpoints: {journey['touchpoints']}\")\n",
        "        print(f\"  Converted: {journey['converted']}\")\n",
        "\n",
        "    return journeys\n",
        "\n",
        "# Generate data and test journey creation\n",
        "unified_data = create_optimized_data_files()\n",
        "debug_unified_data(unified_data)\n",
        "journeys = manual_customer_journey_creation(unified_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrpNoOE8iRSP",
        "outputId": "70adc09e-b01b-47ab-a834-73fc03a53c85"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating optimized data files...\n",
            "Saved unified_data to simulated_data/unified_data.csv\n",
            "Saved website_analytics to simulated_data/website_analytics.csv\n",
            "Saved email_campaigns to simulated_data/email_campaigns.csv\n",
            "Saved paid_ads to simulated_data/paid_ads.csv\n",
            "\n",
            "--- UNIFIED DATA DIAGNOSTICS ---\n",
            "Data shape: (2850, 11)\n",
            "Data types:\n",
            "customer_id              object\n",
            "timestamp        datetime64[ns]\n",
            "data_source              object\n",
            "is_conversion              bool\n",
            "cost                    float64\n",
            "platform                 object\n",
            "campaign                 object\n",
            "page_path                object\n",
            "session_id               object\n",
            "campaign_name            object\n",
            "subject                  object\n",
            "dtype: object\n",
            "\n",
            "Sample data (first 3 rows):\n",
            "  customer_id           timestamp        data_source  is_conversion      cost  \\\n",
            "0   cust_0000 2023-01-01 13:27:00           paid_ads          False  0.936197   \n",
            "2   cust_0000 2023-03-06 16:34:00           paid_ads          False  1.527457   \n",
            "1   cust_0000 2023-03-29 15:05:00  website_analytics          False  1.228635   \n",
            "\n",
            "       platform     campaign page_path        session_id campaign_name subject  \n",
            "0  facebook_ads  acquisition       NaN               NaN           NaN     NaN  \n",
            "2  facebook_ads  acquisition       NaN               NaN           NaN     NaN  \n",
            "1           NaN          NaN      home  session_5913c560           NaN     NaN  \n",
            "\n",
            "Column 'customer_id' stats:\n",
            "  Type: object\n",
            "  Sample values: ['cust_0000', 'cust_0000', 'cust_0000']\n",
            "  Unique count: 500\n",
            "\n",
            "Column 'timestamp' stats:\n",
            "  Type: datetime (good!)\n",
            "  Min: 2023-01-01 00:38:00\n",
            "  Max: 2023-04-01 22:43:00\n",
            "\n",
            "Column 'data_source' stats:\n",
            "  Type: object\n",
            "  Sample values: ['paid_ads', 'paid_ads', 'website_analytics']\n",
            "  Unique count: 3\n",
            "\n",
            "Column 'is_conversion' stats:\n",
            "  Type: bool\n",
            "  Unique values: [False, True]\n",
            "\n",
            "NaN counts:\n",
            "platform         1852\n",
            "campaign         1852\n",
            "page_path        1979\n",
            "session_id       1979\n",
            "campaign_name    1869\n",
            "subject          1869\n",
            "dtype: int64\n",
            "\n",
            "Manually creating customer journeys...\n",
            "Manually created 500 customer journeys\n",
            "\n",
            "Sample journeys (first 5):\n",
            "\n",
            "Journey 1:\n",
            "  Customer: cust_0000\n",
            "  Touchpoints: ['paid_ads', 'paid_ads', 'website_analytics']\n",
            "  Converted: False\n",
            "\n",
            "Journey 2:\n",
            "  Customer: cust_0001\n",
            "  Touchpoints: ['email_campaigns', 'email_campaigns', 'paid_ads', 'paid_ads', 'website_analytics', 'website_analytics', 'email_campaigns']\n",
            "  Converted: False\n",
            "\n",
            "Journey 3:\n",
            "  Customer: cust_0002\n",
            "  Touchpoints: ['paid_ads', 'paid_ads', 'email_campaigns', 'paid_ads']\n",
            "  Converted: False\n",
            "\n",
            "Journey 4:\n",
            "  Customer: cust_0003\n",
            "  Touchpoints: ['website_analytics', 'email_campaigns', 'email_campaigns', 'email_campaigns', 'email_campaigns', 'email_campaigns']\n",
            "  Converted: False\n",
            "\n",
            "Journey 5:\n",
            "  Customer: cust_0004\n",
            "  Touchpoints: ['paid_ads', 'email_campaigns', 'email_campaigns', 'paid_ads', 'paid_ads', 'paid_ads', 'paid_ads']\n",
            "  Converted: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_visualization_issues(attribution_system, journeys, touchpoint_sequence_df=None):\n",
        "    \"\"\"\n",
        "    Fix visualization issues by directly setting necessary components.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Applying visualization fixes ---\")\n",
        "\n",
        "    # 1. Ensure customer_journey has the necessary attributes\n",
        "    if attribution_system.customer_journey.journeys is None:\n",
        "        print(\"Setting journeys in customer_journey component...\")\n",
        "        attribution_system.customer_journey.journeys = journeys\n",
        "\n",
        "    # 2. Create touchpoint_sequence if not provided\n",
        "    if touchpoint_sequence_df is None and attribution_system.customer_journey.touchpoint_sequence is None:\n",
        "        print(\"Creating touchpoint_sequence...\")\n",
        "        touchpoint_sequences = []\n",
        "\n",
        "        for customer_id, journey in journeys.items():\n",
        "            touchpoints = journey['touchpoints']\n",
        "            converted = journey['converted']\n",
        "\n",
        "            sequence = {\n",
        "                'customer_id': customer_id,\n",
        "                'sequence': '->'.join(touchpoints),\n",
        "                'touchpoint_count': len(touchpoints),\n",
        "                'converted': converted\n",
        "            }\n",
        "            touchpoint_sequences.append(sequence)\n",
        "\n",
        "        import pandas as pd\n",
        "        touchpoint_sequence_df = pd.DataFrame(touchpoint_sequences)\n",
        "\n",
        "    # 3. Set touchpoint_sequence in customer_journey\n",
        "    if attribution_system.customer_journey.touchpoint_sequence is None:\n",
        "        print(\"Setting touchpoint_sequence in customer_journey component...\")\n",
        "        attribution_system.customer_journey.touchpoint_sequence = touchpoint_sequence_df\n",
        "\n",
        "    # 4. Create conversion_rates if needed\n",
        "    if attribution_system.customer_journey.conversion_rates is None:\n",
        "        print(\"Creating conversion_rates...\")\n",
        "        # Calculate conversion rates by sequence\n",
        "        sequence_stats = touchpoint_sequence_df.groupby('sequence').agg(\n",
        "            total_customers=pd.NamedAgg(column='customer_id', aggfunc='count'),\n",
        "            conversions=pd.NamedAgg(column='converted', aggfunc='sum')\n",
        "        )\n",
        "        sequence_stats['conversion_rate'] = sequence_stats['conversions'] / sequence_stats['total_customers']\n",
        "        attribution_system.customer_journey.conversion_rates = sequence_stats.sort_values('total_customers', ascending=False)\n",
        "\n",
        "    # 5. Set journeys for attribution_models\n",
        "    print(\"Setting journeys in attribution_models component...\")\n",
        "    attribution_system.attribution_models.set_journeys(journeys)\n",
        "\n",
        "    # 6. Set data for advanced_analytics\n",
        "    print(\"Setting data in advanced_analytics component...\")\n",
        "    attribution_system.advanced_analytics.set_data(journeys=journeys)\n",
        "\n",
        "    # 7. Set components for visualization\n",
        "    print(\"Setting components in visualization component...\")\n",
        "    attribution_system.visualization.set_components(\n",
        "        attribution_models=attribution_system.attribution_models,\n",
        "        customer_journey=attribution_system.customer_journey,\n",
        "        advanced_analytics=attribution_system.advanced_analytics\n",
        "    )\n",
        "\n",
        "    print(\"Visualization fixes applied successfully!\")\n",
        "    return True"
      ],
      "metadata": {
        "id": "XyHgabDJpo3e"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Advanced Marketing Attribution Models Package\n",
        "==============================================\n",
        "\n",
        "This package implements a comprehensive marketing attribution system with multiple models,\n",
        "from traditional to cutting-edge approaches, along with data ingestion, processing,\n",
        "and visualization capabilities.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import networkx as nx\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import roc_auc_score, precision_recall_curve, average_precision_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout, Input, Attention, MultiHeadAttention\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import shap\n",
        "from itertools import combinations, permutations\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "import json\n",
        "import os\n",
        "import datetime\n",
        "import pickle\n",
        "import logging\n",
        "import random\n",
        "import uuid\n",
        "import os\n",
        "import datetime\n",
        "from typing import Dict, List, Tuple, Optional, Union, Any, Callable\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger('marketing_attribution')\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class DataIngestion:\n",
        "    \"\"\"\n",
        "    Component for ingesting and integrating data from multiple marketing channels.\n",
        "    Supports various data sources and formats.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: Dict[str, Any] = None):\n",
        "        \"\"\"\n",
        "        Initialize the data ingestion component.\n",
        "\n",
        "        Args:\n",
        "            config: Configuration parameters for data sources\n",
        "        \"\"\"\n",
        "        self.config = config or {}\n",
        "        self.data_sources = {}\n",
        "        self.unified_data = None\n",
        "        logger.info(\"DataIngestion component initialized\")\n",
        "\n",
        "    def add_data_source(self, name: str, source_type: str, connection_params: Dict[str, Any] = None):\n",
        "        \"\"\"\n",
        "        Add a data source for ingestion.\n",
        "\n",
        "        Args:\n",
        "            name: Unique identifier for the data source\n",
        "            source_type: Type of data source (e.g., 'csv', 'api', 'sql', 'ga', 'facebook', etc.)\n",
        "            connection_params: Parameters required to connect to the data source\n",
        "        \"\"\"\n",
        "        self.data_sources[name] = {\n",
        "            'type': source_type,\n",
        "            'params': connection_params or {},\n",
        "            'data': None\n",
        "        }\n",
        "        logger.info(f\"Added data source: {name} of type {source_type}\")\n",
        "\n",
        "    def ingest_csv(self, name: str, file_path: str, **kwargs):\n",
        "        \"\"\"\n",
        "        Ingest data from a CSV file.\n",
        "\n",
        "        Args:\n",
        "            name: Name of the data source\n",
        "            file_path: Path to the CSV file\n",
        "            **kwargs: Additional parameters for pandas.read_csv\n",
        "        \"\"\"\n",
        "        try:\n",
        "            data = pd.read_csv(file_path, **kwargs)\n",
        "            if name not in self.data_sources:\n",
        "                self.add_data_source(name, 'csv')\n",
        "            self.data_sources[name]['data'] = data\n",
        "            self.data_sources[name]['params']['file_path'] = file_path\n",
        "            logger.info(f\"Successfully ingested CSV data from {file_path} as {name}\")\n",
        "            return data\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error ingesting CSV data from {file_path}: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def ingest_api_data(self, name: str, api_url: str, headers: Dict = None, params: Dict = None):\n",
        "        \"\"\"\n",
        "        Ingest data from an API endpoint.\n",
        "\n",
        "        Args:\n",
        "            name: Name of the data source\n",
        "            api_url: URL of the API endpoint\n",
        "            headers: HTTP headers for the request\n",
        "            params: Query parameters for the request\n",
        "        \"\"\"\n",
        "        import requests\n",
        "\n",
        "        try:\n",
        "            response = requests.get(api_url, headers=headers, params=params)\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            # Convert to DataFrame if the data is structured appropriately\n",
        "            if isinstance(data, list) and len(data) > 0:\n",
        "                df = pd.DataFrame(data)\n",
        "            elif isinstance(data, dict) and 'results' in data:\n",
        "                df = pd.DataFrame(data['results'])\n",
        "            else:\n",
        "                df = pd.DataFrame([data])\n",
        "\n",
        "            if name not in self.data_sources:\n",
        "                self.add_data_source(name, 'api')\n",
        "            self.data_sources[name]['data'] = df\n",
        "            self.data_sources[name]['params'].update({\n",
        "                'api_url': api_url,\n",
        "                'headers': headers,\n",
        "                'params': params\n",
        "            })\n",
        "            logger.info(f\"Successfully ingested API data from {api_url} as {name}\")\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error ingesting API data from {api_url}: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def ingest_sql_data(self, name: str, query: str, connection_string: str, **kwargs):\n",
        "        \"\"\"\n",
        "        Ingest data from a SQL database.\n",
        "\n",
        "        Args:\n",
        "            name: Name of the data source\n",
        "            query: SQL query to execute\n",
        "            connection_string: Database connection string\n",
        "            **kwargs: Additional parameters for sqlalchemy\n",
        "        \"\"\"\n",
        "        from sqlalchemy import create_engine\n",
        "\n",
        "        try:\n",
        "            engine = create_engine(connection_string)\n",
        "            data = pd.read_sql(query, engine, **kwargs)\n",
        "\n",
        "            if name not in self.data_sources:\n",
        "                self.add_data_source(name, 'sql')\n",
        "            self.data_sources[name]['data'] = data\n",
        "            self.data_sources[name]['params'].update({\n",
        "                'query': query,\n",
        "                'connection_string': connection_string\n",
        "            })\n",
        "            logger.info(f\"Successfully ingested SQL data as {name}\")\n",
        "            return data\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error ingesting SQL data: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def ingest_google_analytics(self, name: str, view_id: str, metrics: List[str],\n",
        "                              dimensions: List[str], start_date: str, end_date: str,\n",
        "                              credentials_path: str = None):\n",
        "        \"\"\"\n",
        "        Ingest data from Google Analytics.\n",
        "\n",
        "        Args:\n",
        "            name: Name of the data source\n",
        "            view_id: Google Analytics view ID\n",
        "            metrics: List of metrics to retrieve\n",
        "            dimensions: List of dimensions to retrieve\n",
        "            start_date: Start date in YYYY-MM-DD format\n",
        "            end_date: End date in YYYY-MM-DD format\n",
        "            credentials_path: Path to the service account credentials file\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # This requires the google-analytics-data library\n",
        "            # pip install google-analytics-data\n",
        "            from google.analytics.data_v1beta import BetaAnalyticsDataClient\n",
        "            from google.analytics.data_v1beta.types import RunReportRequest, Dimension, Metric\n",
        "\n",
        "            client = BetaAnalyticsDataClient.from_service_account_json(credentials_path) if credentials_path else BetaAnalyticsDataClient()\n",
        "\n",
        "            dimension_list = [Dimension(name=dim) for dim in dimensions]\n",
        "            metric_list = [Metric(name=met) for met in metrics]\n",
        "\n",
        "            request = RunReportRequest(\n",
        "                property=f\"properties/{view_id}\",\n",
        "                dimensions=dimension_list,\n",
        "                metrics=metric_list,\n",
        "                date_ranges=[{\"start_date\": start_date, \"end_date\": end_date}]\n",
        "            )\n",
        "\n",
        "            response = client.run_report(request)\n",
        "\n",
        "            # Convert the response to a pandas DataFrame\n",
        "            rows = []\n",
        "            for row in response.rows:\n",
        "                row_data = {}\n",
        "                for i, dimension in enumerate(row.dimension_values):\n",
        "                    row_data[dimensions[i]] = dimension.value\n",
        "                for i, metric in enumerate(row.metric_values):\n",
        "                    row_data[metrics[i]] = float(metric.value)\n",
        "                rows.append(row_data)\n",
        "\n",
        "            df = pd.DataFrame(rows)\n",
        "\n",
        "            if name not in self.data_sources:\n",
        "                self.add_data_source(name, 'google_analytics')\n",
        "            self.data_sources[name]['data'] = df\n",
        "            self.data_sources[name]['params'].update({\n",
        "                'view_id': view_id,\n",
        "                'metrics': metrics,\n",
        "                'dimensions': dimensions,\n",
        "                'start_date': start_date,\n",
        "                'end_date': end_date\n",
        "            })\n",
        "            logger.info(f\"Successfully ingested Google Analytics data as {name}\")\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error ingesting Google Analytics data: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def unify_customer_data(self, id_mapping_columns: Dict[str, str],\n",
        "                           timestamp_columns: Dict[str, str] = None,\n",
        "                           conversion_column: str = None):\n",
        "        \"\"\"\n",
        "        Unify data from multiple sources using customer identifiers.\n",
        "\n",
        "        Args:\n",
        "            id_mapping_columns: Dict mapping data source names to their ID column names\n",
        "            timestamp_columns: Dict mapping data source names to their timestamp column names\n",
        "            conversion_column: Column name that indicates conversion events\n",
        "        \"\"\"\n",
        "        unified_data_frames = []\n",
        "\n",
        "        for source_name, source_info in self.data_sources.items():\n",
        "            if source_info['data'] is None:\n",
        "                logger.warning(f\"Skipping {source_name} as it has no data\")\n",
        "                continue\n",
        "\n",
        "            df = source_info['data'].copy()\n",
        "\n",
        "            # Add source identifier\n",
        "            df['data_source'] = source_name\n",
        "\n",
        "            # Rename ID column to a common name\n",
        "            if source_name in id_mapping_columns:\n",
        "                id_col = id_mapping_columns[source_name]\n",
        "                if id_col in df.columns:\n",
        "                    df.rename(columns={id_col: 'customer_id'}, inplace=True)\n",
        "                else:\n",
        "                    logger.warning(f\"ID column {id_col} not found in {source_name}\")\n",
        "\n",
        "            # Standardize timestamp columns\n",
        "            if timestamp_columns and source_name in timestamp_columns:\n",
        "                ts_col = timestamp_columns[source_name]\n",
        "                if ts_col in df.columns:\n",
        "                    df.rename(columns={ts_col: 'timestamp'}, inplace=True)\n",
        "                    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "                else:\n",
        "                    logger.warning(f\"Timestamp column {ts_col} not found in {source_name}\")\n",
        "\n",
        "            unified_data_frames.append(df)\n",
        "\n",
        "        # Combine all data frames\n",
        "        if unified_data_frames:\n",
        "            self.unified_data = pd.concat(unified_data_frames, ignore_index=True)\n",
        "\n",
        "            # Sort by customer_id and timestamp if available\n",
        "            sort_cols = ['customer_id']\n",
        "            if 'timestamp' in self.unified_data.columns:\n",
        "                sort_cols.append('timestamp')\n",
        "            self.unified_data.sort_values(sort_cols, inplace=True)\n",
        "\n",
        "            # Flag conversion events if specified\n",
        "            if conversion_column and conversion_column in self.unified_data.columns:\n",
        "                self.unified_data['is_conversion'] = self.unified_data[conversion_column].notna() & (self.unified_data[conversion_column] != 0)\n",
        "\n",
        "            logger.info(f\"Unified data created with {len(self.unified_data)} rows\")\n",
        "        else:\n",
        "            logger.warning(\"No data sources with data to unify\")\n",
        "\n",
        "        return self.unified_data\n",
        "\n",
        "\n",
        "class CustomerJourney:\n",
        "    \"\"\"\n",
        "    Component for modeling and analyzing customer journeys across touchpoints.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data: pd.DataFrame = None):\n",
        "        \"\"\"\n",
        "        Initialize the customer journey component.\n",
        "\n",
        "        Args:\n",
        "            data: Unified customer data from DataIngestion\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.journeys = None\n",
        "        self.touchpoint_sequence = None\n",
        "        self.conversion_rates = None\n",
        "        logger.info(\"CustomerJourney component initialized\")\n",
        "\n",
        "    def set_data(self, data: pd.DataFrame):\n",
        "        \"\"\"\n",
        "        Set the data for customer journey analysis.\n",
        "\n",
        "        Args:\n",
        "            data: Unified customer data\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        logger.info(\"Data set for customer journey analysis\")\n",
        "\n",
        "    def create_customer_journeys(self, customer_id_col: str = 'customer_id',\n",
        "                               timestamp_col: str = 'timestamp',\n",
        "                               channel_col: str = 'data_source',\n",
        "                               conversion_col: str = 'is_conversion'):\n",
        "        \"\"\"\n",
        "        Create customer journeys by sequencing touchpoints chronologically.\n",
        "\n",
        "        Args:\n",
        "            customer_id_col: Column name for customer identifier\n",
        "            timestamp_col: Column name for timestamp\n",
        "            channel_col: Column name for marketing channel or touchpoint\n",
        "            conversion_col: Column name for conversion indicator\n",
        "        \"\"\"\n",
        "        if self.data is None:\n",
        "            logger.error(\"No data available for creating customer journeys\")\n",
        "            return None\n",
        "\n",
        "        required_cols = [customer_id_col, channel_col]\n",
        "        if timestamp_col and timestamp_col in self.data.columns:\n",
        "            required_cols.append(timestamp_col)\n",
        "\n",
        "        # Check if required columns exist\n",
        "        missing_cols = [col for col in required_cols if col not in self.data.columns]\n",
        "        if missing_cols:\n",
        "            logger.error(f\"Missing required columns: {missing_cols}\")\n",
        "            return None\n",
        "\n",
        "        # Prepare the data\n",
        "        journey_data = self.data[required_cols].copy()\n",
        "\n",
        "        # Add conversion flag if available\n",
        "        if conversion_col and conversion_col in self.data.columns:\n",
        "            journey_data[conversion_col] = self.data[conversion_col]\n",
        "        else:\n",
        "            journey_data[conversion_col] = False\n",
        "\n",
        "        # Sort by customer ID and timestamp\n",
        "        sort_cols = [customer_id_col]\n",
        "        if timestamp_col and timestamp_col in journey_data.columns:\n",
        "            journey_data[timestamp_col] = pd.to_datetime(journey_data[timestamp_col])\n",
        "            sort_cols.append(timestamp_col)\n",
        "\n",
        "        journey_data = journey_data.sort_values(sort_cols)\n",
        "\n",
        "        # Group by customer ID to create journeys\n",
        "        journeys = {}\n",
        "        touchpoint_sequences = []\n",
        "\n",
        "        for customer_id, group in journey_data.groupby(customer_id_col):\n",
        "            touchpoints = group[channel_col].tolist()\n",
        "            timestamps = group[timestamp_col].tolist() if timestamp_col in group.columns else None\n",
        "            converted = any(group[conversion_col]) if conversion_col in group.columns else False\n",
        "\n",
        "            journey = {\n",
        "                'customer_id': customer_id,\n",
        "                'touchpoints': touchpoints,\n",
        "                'timestamps': timestamps,\n",
        "                'converted': converted\n",
        "            }\n",
        "\n",
        "            journeys[customer_id] = journey\n",
        "\n",
        "            # Record the touchpoint sequence\n",
        "            sequence = {\n",
        "                'customer_id': customer_id,\n",
        "                'sequence': '->'.join(touchpoints),\n",
        "                'touchpoint_count': len(touchpoints),\n",
        "                'converted': converted\n",
        "            }\n",
        "            touchpoint_sequences.append(sequence)\n",
        "\n",
        "        self.journeys = journeys\n",
        "        self.touchpoint_sequence = pd.DataFrame(touchpoint_sequences)\n",
        "\n",
        "        # Calculate conversion rates by sequence\n",
        "        sequence_stats = self.touchpoint_sequence.groupby('sequence').agg(\n",
        "            total_customers=pd.NamedAgg(column='customer_id', aggfunc='count'),\n",
        "            conversions=pd.NamedAgg(column='converted', aggfunc='sum')\n",
        "        )\n",
        "        sequence_stats['conversion_rate'] = sequence_stats['conversions'] / sequence_stats['total_customers']\n",
        "        self.conversion_rates = sequence_stats.sort_values('total_customers', ascending=False)\n",
        "\n",
        "        logger.info(f\"Created {len(journeys)} customer journeys\")\n",
        "        return self.journeys\n",
        "\n",
        "    def get_most_common_journeys(self, top_n: int = 10):\n",
        "        \"\"\"\n",
        "        Get the most common customer journeys.\n",
        "\n",
        "        Args:\n",
        "            top_n: Number of top journeys to return\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with top journeys and their statistics\n",
        "        \"\"\"\n",
        "        if self.touchpoint_sequence is None:\n",
        "            logger.warning(\"No touchpoint sequences available. Call create_customer_journeys first.\")\n",
        "            return None\n",
        "\n",
        "        journey_counts = self.touchpoint_sequence['sequence'].value_counts().reset_index()\n",
        "        journey_counts.columns = ['journey', 'count']\n",
        "\n",
        "        # Add conversion rates\n",
        "        journey_stats = journey_counts.merge(\n",
        "            self.touchpoint_sequence.groupby('sequence')['converted'].mean().reset_index(),\n",
        "            left_on='journey',\n",
        "            right_on='sequence'\n",
        "        )\n",
        "        journey_stats.drop('sequence', axis=1, inplace=True)\n",
        "        journey_stats.rename(columns={'converted': 'conversion_rate'}, inplace=True)\n",
        "\n",
        "        return journey_stats.head(top_n)\n",
        "\n",
        "    def add_funnel_stages(self, stage_mapping: Dict[str, str]):\n",
        "        \"\"\"\n",
        "        Add funnel stage information to customer journeys.\n",
        "\n",
        "        Args:\n",
        "            stage_mapping: Mapping from touchpoint/channel to funnel stage\n",
        "                          (e.g., {'facebook_ad': 'awareness', 'email': 'consideration', ...})\n",
        "        \"\"\"\n",
        "        if self.data is None or self.journeys is None:\n",
        "            logger.warning(\"No data or journeys available\")\n",
        "            return\n",
        "\n",
        "        # Add stage column to the original data\n",
        "        channel_col = 'data_source'  # Adjust as needed\n",
        "        self.data['funnel_stage'] = self.data[channel_col].map(stage_mapping)\n",
        "\n",
        "        # Update journeys with stage information\n",
        "        for customer_id, journey in self.journeys.items():\n",
        "            stages = [stage_mapping.get(touchpoint, 'unknown') for touchpoint in journey['touchpoints']]\n",
        "            self.journeys[customer_id]['stages'] = stages\n",
        "\n",
        "        logger.info(\"Added funnel stages to customer journeys\")\n",
        "\n",
        "    def calculate_time_between_touchpoints(self):\n",
        "        \"\"\"\n",
        "        Calculate the time between consecutive touchpoints in customer journeys.\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with time differences between touchpoints\n",
        "        \"\"\"\n",
        "        if self.journeys is None or 'timestamps' not in next(iter(self.journeys.values())):\n",
        "            logger.warning(\"No journey timestamps available\")\n",
        "            return None\n",
        "\n",
        "        time_diffs = []\n",
        "\n",
        "        for customer_id, journey in self.journeys.items():\n",
        "            touchpoints = journey['touchpoints']\n",
        "            timestamps = journey['timestamps']\n",
        "\n",
        "            if len(touchpoints) < 2 or timestamps is None:\n",
        "                continue\n",
        "\n",
        "            for i in range(1, len(touchpoints)):\n",
        "                time_diff = timestamps[i] - timestamps[i-1]\n",
        "                time_diff_hours = time_diff.total_seconds() / 3600\n",
        "\n",
        "                time_diffs.append({\n",
        "                    'customer_id': customer_id,\n",
        "                    'from_touchpoint': touchpoints[i-1],\n",
        "                    'to_touchpoint': touchpoints[i],\n",
        "                    'time_diff_hours': time_diff_hours\n",
        "                })\n",
        "\n",
        "        if time_diffs:\n",
        "            df_time_diffs = pd.DataFrame(time_diffs)\n",
        "            return df_time_diffs\n",
        "\n",
        "        return None\n",
        "\n",
        "\n",
        "class AttributionModels:\n",
        "    \"\"\"\n",
        "    Component implementing various attribution models from traditional to advanced.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, journeys=None):\n",
        "        \"\"\"\n",
        "        Initialize the attribution models component.\n",
        "\n",
        "        Args:\n",
        "            journeys: Customer journeys from CustomerJourney component\n",
        "        \"\"\"\n",
        "        self.journeys = journeys\n",
        "        self.results = {}\n",
        "        self.channel_contribution = {}\n",
        "        logger.info(\"AttributionModels component initialized\")\n",
        "\n",
        "    def set_journeys(self, journeys):\n",
        "        \"\"\"\n",
        "        Set the customer journeys for attribution.\n",
        "\n",
        "        Args:\n",
        "            journeys: Customer journeys\n",
        "        \"\"\"\n",
        "        self.journeys = journeys\n",
        "        logger.info(\"Customer journeys set for attribution modeling\")\n",
        "\n",
        "    def _prepare_touchpoint_data(self):\n",
        "        \"\"\"\n",
        "        Prepare touchpoint data for attribution modeling.\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with touchpoint data\n",
        "        \"\"\"\n",
        "        if not self.journeys:\n",
        "            logger.error(\"No customer journeys available\")\n",
        "            return None\n",
        "\n",
        "        touchpoint_data = []\n",
        "\n",
        "        for customer_id, journey in self.journeys.items():\n",
        "            touchpoints = journey['touchpoints']\n",
        "            converted = journey.get('converted', False)\n",
        "\n",
        "            for i, touchpoint in enumerate(touchpoints):\n",
        "                position = i + 1\n",
        "                is_first = (i == 0)\n",
        "                is_last = (i == len(touchpoints) - 1)\n",
        "\n",
        "                touchpoint_data.append({\n",
        "                    'customer_id': customer_id,\n",
        "                    'touchpoint': touchpoint,\n",
        "                    'position': position,\n",
        "                    'is_first': is_first,\n",
        "                    'is_last': is_last,\n",
        "                    'total_touchpoints': len(touchpoints),\n",
        "                    'converted': converted\n",
        "                })\n",
        "\n",
        "        return pd.DataFrame(touchpoint_data)\n",
        "\n",
        "    def run_first_touch_attribution(self):\n",
        "        \"\"\"\n",
        "        Run first-touch attribution model.\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with attribution results\n",
        "        \"\"\"\n",
        "        touchpoint_data = self._prepare_touchpoint_data()\n",
        "        if touchpoint_data is None:\n",
        "            return None\n",
        "\n",
        "        # Filter for converted journeys and first touchpoints\n",
        "        first_touch = touchpoint_data[\n",
        "            (touchpoint_data['converted'] == True) &\n",
        "            (touchpoint_data['is_first'] == True)\n",
        "        ]\n",
        "\n",
        "        # Count conversions by touchpoint\n",
        "        attribution = first_touch['touchpoint'].value_counts().reset_index()\n",
        "        attribution.columns = ['touchpoint', 'attributed_conversions']\n",
        "\n",
        "        # Calculate percentage\n",
        "        total_conversions = attribution['attributed_conversions'].sum()\n",
        "        attribution['attribution_percentage'] = (attribution['attributed_conversions'] / total_conversions) * 100\n",
        "\n",
        "        self.results['first_touch'] = attribution\n",
        "        self.channel_contribution['first_touch'] = dict(zip(attribution['touchpoint'], attribution['attribution_percentage']))\n",
        "\n",
        "        logger.info(\"First-touch attribution model executed\")\n",
        "        return attribution\n",
        "\n",
        "    def run_last_touch_attribution(self):\n",
        "        \"\"\"\n",
        "        Run last-touch attribution model.\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with attribution results\n",
        "        \"\"\"\n",
        "        touchpoint_data = self._prepare_touchpoint_data()\n",
        "        if touchpoint_data is None:\n",
        "            return None\n",
        "\n",
        "        # Filter for converted journeys and last touchpoints\n",
        "        last_touch = touchpoint_data[\n",
        "            (touchpoint_data['converted'] == True) &\n",
        "            (touchpoint_data['is_last'] == True)\n",
        "        ]\n",
        "\n",
        "        # Count conversions by touchpoint\n",
        "        attribution = last_touch['touchpoint'].value_counts().reset_index()\n",
        "        attribution.columns = ['touchpoint', 'attributed_conversions']\n",
        "\n",
        "        # Calculate percentage\n",
        "        total_conversions = attribution['attributed_conversions'].sum()\n",
        "        attribution['attribution_percentage'] = (attribution['attributed_conversions'] / total_conversions) * 100\n",
        "\n",
        "        self.results['last_touch'] = attribution\n",
        "        self.channel_contribution['last_touch'] = dict(zip(attribution['touchpoint'], attribution['attribution_percentage']))\n",
        "\n",
        "        logger.info(\"Last-touch attribution model executed\")\n",
        "        return attribution\n",
        "\n",
        "    def run_linear_attribution(self):\n",
        "        \"\"\"\n",
        "        Run linear attribution model where credit is distributed equally across touchpoints.\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with attribution results\n",
        "        \"\"\"\n",
        "        touchpoint_data = self._prepare_touchpoint_data()\n",
        "        if touchpoint_data is None:\n",
        "            return None\n",
        "\n",
        "        # Filter for converted journeys\n",
        "        converted_journeys = touchpoint_data[touchpoint_data['converted'] == True]\n",
        "\n",
        "        # Calculate linear attribution (each touchpoint gets 1/n credit)\n",
        "        converted_journeys['linear_credit'] = 1 / converted_journeys['total_touchpoints']\n",
        "\n",
        "        # Sum up credits by touchpoint\n",
        "        attribution = converted_journeys.groupby('touchpoint')['linear_credit'].sum().reset_index()\n",
        "        attribution.columns = ['touchpoint', 'attributed_conversions']\n",
        "\n",
        "        # Calculate percentage\n",
        "        total_attributions = attribution['attributed_conversions'].sum()\n",
        "        attribution['attribution_percentage'] = (attribution['attributed_conversions'] / total_attributions) * 100\n",
        "\n",
        "        self.results['linear'] = attribution\n",
        "        self.channel_contribution['linear'] = dict(zip(attribution['touchpoint'], attribution['attribution_percentage']))\n",
        "\n",
        "        logger.info(\"Linear attribution model executed\")\n",
        "        return attribution\n",
        "\n",
        "    def run_position_based_attribution(self, first_weight=0.4, last_weight=0.4):\n",
        "        \"\"\"\n",
        "        Run position-based attribution model (e.g., U-shaped).\n",
        "\n",
        "        Args:\n",
        "            first_weight: Weight for the first touchpoint (default: 0.4)\n",
        "            last_weight: Weight for the last touchpoint (default: 0.4)\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with attribution results\n",
        "        \"\"\"\n",
        "        touchpoint_data = self._prepare_touchpoint_data()\n",
        "        if touchpoint_data is None:\n",
        "            return None\n",
        "\n",
        "        # Filter for converted journeys\n",
        "        converted_journeys = touchpoint_data[touchpoint_data['converted'] == True]\n",
        "\n",
        "        # Calculate middle weight (remaining weight divided by number of middle touchpoints)\n",
        "        middle_weight = 1 - first_weight - last_weight\n",
        "\n",
        "        # Assign credits based on position\n",
        "        def assign_position_credit(row):\n",
        "            if row['total_touchpoints'] == 1:\n",
        "                return 1.0  # If only one touchpoint, it gets full credit\n",
        "            elif row['is_first']:\n",
        "                return first_weight\n",
        "            elif row['is_last']:\n",
        "                return last_weight\n",
        "            else:\n",
        "                # Distribute middle weight among middle touchpoints\n",
        "                middle_touchpoints = row['total_touchpoints'] - 2\n",
        "                return middle_weight / middle_touchpoints if middle_touchpoints > 0 else 0\n",
        "\n",
        "        converted_journeys['position_credit'] = converted_journeys.apply(assign_position_credit, axis=1)\n",
        "\n",
        "        # Sum up credits by touchpoint\n",
        "        attribution = converted_journeys.groupby('touchpoint')['position_credit'].sum().reset_index()\n",
        "        attribution.columns = ['touchpoint', 'attributed_conversions']\n",
        "\n",
        "        # Calculate percentage\n",
        "        total_attributions = attribution['attributed_conversions'].sum()\n",
        "        attribution['attribution_percentage'] = (attribution['attributed_conversions'] / total_attributions) * 100\n",
        "\n",
        "        model_name = f\"position_based_{first_weight}_{last_weight}\"\n",
        "        self.results[model_name] = attribution\n",
        "        self.channel_contribution[model_name] = dict(zip(attribution['touchpoint'], attribution['attribution_percentage']))\n",
        "\n",
        "        logger.info(f\"Position-based attribution model executed with first weight {first_weight}, last weight {last_weight}\")\n",
        "        return attribution\n",
        "\n",
        "    def run_time_decay_attribution(self, half_life_days=7):\n",
        "        \"\"\"\n",
        "        Run time-decay attribution model where recent touchpoints get more credit.\n",
        "\n",
        "        Args:\n",
        "            half_life_days: Number of days after which the credit is halved (default: 7)\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with attribution results\n",
        "        \"\"\"\n",
        "        if not self.journeys or 'timestamps' not in next(iter(self.journeys.values()), {}):\n",
        "            logger.warning(\"No journey timestamps available for time decay model\")\n",
        "            return None\n",
        "\n",
        "        time_decay_data = []\n",
        "\n",
        "        for customer_id, journey in self.journeys.items():\n",
        "            touchpoints = journey['touchpoints']\n",
        "            timestamps = journey['timestamps']\n",
        "            converted = journey.get('converted', False)\n",
        "\n",
        "            if not converted or not timestamps:\n",
        "                continue\n",
        "\n",
        "            # Get the conversion time (the last timestamp)\n",
        "            conversion_time = timestamps[-1]\n",
        "\n",
        "            for i, (touchpoint, timestamp) in enumerate(zip(touchpoints, timestamps)):\n",
        "                # Calculate days before conversion\n",
        "                days_before_conversion = (conversion_time - timestamp).total_seconds() / (24 * 3600)\n",
        "\n",
        "                # Calculate decay factor\n",
        "                decay_factor = 2 ** (-days_before_conversion / half_life_days)\n",
        "\n",
        "                time_decay_data.append({\n",
        "                    'customer_id': customer_id,\n",
        "                    'touchpoint': touchpoint,\n",
        "                    'days_before_conversion': days_before_conversion,\n",
        "                    'decay_factor': decay_factor\n",
        "                })\n",
        "\n",
        "        if not time_decay_data:\n",
        "            logger.warning(\"No data available for time decay attribution\")\n",
        "            return None\n",
        "\n",
        "        df_time_decay = pd.DataFrame(time_decay_data)\n",
        "\n",
        "        # Calculate normalized credits per customer journey\n",
        "        customer_totals = df_time_decay.groupby('customer_id')['decay_factor'].sum()\n",
        "        df_time_decay = df_time_decay.merge(\n",
        "            customer_totals.reset_index().rename(columns={'decay_factor': 'total_decay'}),\n",
        "            on='customer_id'\n",
        "        )\n",
        "        df_time_decay['time_decay_credit'] = df_time_decay['decay_factor'] / df_time_decay['total_decay']\n",
        "\n",
        "        # Sum up credits by touchpoint\n",
        "        attribution = df_time_decay.groupby('touchpoint')['time_decay_credit'].sum().reset_index()\n",
        "        attribution.columns = ['touchpoint', 'attributed_conversions']\n",
        "\n",
        "        # Calculate percentage\n",
        "        total_attributions = attribution['attributed_conversions'].sum()\n",
        "        attribution['attribution_percentage'] = (attribution['attributed_conversions'] / total_attributions) * 100\n",
        "\n",
        "        model_name = f\"time_decay_{half_life_days}\"\n",
        "        self.results[model_name] = attribution\n",
        "        self.channel_contribution[model_name] = dict(zip(attribution['touchpoint'], attribution['attribution_percentage']))\n",
        "\n",
        "        logger.info(f\"Time-decay attribution model executed with half-life {half_life_days} days\")\n",
        "        return attribution\n",
        "\n",
        "    def run_markov_chain_attribution(self, transition_threshold=5):\n",
        "        \"\"\"\n",
        "        Run Markov Chain attribution model to estimate channel contribution.\n",
        "\n",
        "        Args:\n",
        "            transition_threshold: Minimum number of transitions required to include in the model\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with attribution results\n",
        "        \"\"\"\n",
        "        if not self.journeys:\n",
        "            logger.error(\"No customer journeys available\")\n",
        "            return None\n",
        "\n",
        "        # Start and end states for the Markov model\n",
        "        start_state = 'START'\n",
        "        conversion_state = 'CONVERSION'\n",
        "        null_state = 'NULL'\n",
        "\n",
        "        # Initialize transition counts\n",
        "        transitions = {}\n",
        "\n",
        "        # Count transitions in customer journeys\n",
        "        for customer_id, journey in self.journeys.items():\n",
        "            touchpoints = journey['touchpoints']\n",
        "            converted = journey.get('converted', False)\n",
        "\n",
        "            if not touchpoints:\n",
        "                continue\n",
        "\n",
        "            # Add start state transition\n",
        "            first_state = touchpoints[0]\n",
        "            transitions.setdefault((start_state, first_state), 0)\n",
        "            transitions[(start_state, first_state)] += 1\n",
        "\n",
        "            # Add transitions between touchpoints\n",
        "            for i in range(len(touchpoints) - 1):\n",
        "                from_state = touchpoints[i]\n",
        "                to_state = touchpoints[i + 1]\n",
        "                transitions.setdefault((from_state, to_state), 0)\n",
        "                transitions[(from_state, to_state)] += 1\n",
        "\n",
        "            # Add conversion or null transition from last touchpoint\n",
        "            last_state = touchpoints[-1]\n",
        "            if converted:\n",
        "                transitions.setdefault((last_state, conversion_state), 0)\n",
        "                transitions[(last_state, conversion_state)] += 1\n",
        "            else:\n",
        "                transitions.setdefault((last_state, null_state), 0)\n",
        "                transitions[(last_state, null_state)] += 1\n",
        "\n",
        "        # Filter transitions based on threshold\n",
        "        transitions = {k: v for k, v in transitions.items() if v >= transition_threshold}\n",
        "\n",
        "        # Create transition matrix\n",
        "        all_states = set()\n",
        "        for from_state, to_state in transitions.keys():\n",
        "            all_states.add(from_state)\n",
        "            all_states.add(to_state)\n",
        "\n",
        "        all_states = list(all_states)\n",
        "        state_indices = {state: idx for idx, state in enumerate(all_states)}\n",
        "\n",
        "        # Initialize transition probability matrix\n",
        "        n_states = len(all_states)\n",
        "        trans_matrix = np.zeros((n_states, n_states))\n",
        "\n",
        "        # Fill in transition probabilities\n",
        "        for (from_state, to_state), count in transitions.items():\n",
        "            if from_state in state_indices and to_state in state_indices:\n",
        "                from_idx = state_indices[from_state]\n",
        "                to_idx = state_indices[to_state]\n",
        "                trans_matrix[from_idx, to_idx] = count\n",
        "\n",
        "        # Normalize transition probabilities\n",
        "        row_sums = trans_matrix.sum(axis=1)\n",
        "        row_sums[row_sums == 0] = 1  # Avoid division by zero\n",
        "        trans_matrix = trans_matrix / row_sums[:, np.newaxis]\n",
        "\n",
        "        # Function to calculate conversion probability\n",
        "        def calculate_conversion_prob(matrix, state_map, conversion_idx):\n",
        "            n = matrix.shape[0]\n",
        "            Q = matrix.copy()\n",
        "\n",
        "            # Remove conversion and null state rows/columns\n",
        "            mask = np.ones(n, dtype=bool)\n",
        "            for state in [conversion_state, null_state]:\n",
        "                if state in state_map:\n",
        "                    mask[state_map[state]] = False\n",
        "\n",
        "            Q = Q[mask][:, mask]\n",
        "\n",
        "            # Calculate fundamental matrix\n",
        "            I = np.identity(Q.shape[0])\n",
        "            N = np.linalg.inv(I - Q)\n",
        "\n",
        "            # Calculate conversion probability from each state\n",
        "            R = matrix.copy()\n",
        "            R = R[mask][:, conversion_idx]\n",
        "\n",
        "            # Conversion probabilities\n",
        "            B = N.dot(R)\n",
        "\n",
        "            return B\n",
        "\n",
        "        # Calculate conversion probability from each state\n",
        "        conversion_idx = state_indices.get(conversion_state)\n",
        "        if conversion_idx is None:\n",
        "            logger.warning(\"No conversion state in transition matrix\")\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            # Conversion probability from each state\n",
        "            conv_probs = calculate_conversion_prob(trans_matrix, state_indices, conversion_idx)\n",
        "\n",
        "            # Calculate removal effect\n",
        "            removal_effects = {}\n",
        "\n",
        "            # Identification of channels (excluding special states)\n",
        "            channels = [state for state in all_states if state not in [start_state, conversion_state, null_state]]\n",
        "\n",
        "            for channel in channels:\n",
        "                # Create a new transition matrix with the channel removed\n",
        "                adj_matrix = trans_matrix.copy()\n",
        "                channel_idx = state_indices.get(channel)\n",
        "\n",
        "                if channel_idx is not None:\n",
        "                    # Remove the channel by setting its column to 0\n",
        "                    adj_matrix[:, channel_idx] = 0\n",
        "\n",
        "                    # Calculate new conversion probability\n",
        "                    try:\n",
        "                        adj_conv_prob = calculate_conversion_prob(adj_matrix, state_indices, conversion_idx)\n",
        "                        start_idx = state_indices.get(start_state)\n",
        "\n",
        "                        if start_idx is not None and start_idx < len(conv_probs):\n",
        "                            # Calculate removal effect\n",
        "                            original_prob = conv_probs[start_idx]\n",
        "                            new_prob = adj_conv_prob[start_idx] if start_idx < len(adj_conv_prob) else 0\n",
        "                            removal_effect = original_prob - new_prob\n",
        "                            removal_effects[channel] = max(0, removal_effect)\n",
        "                    except Exception as e:\n",
        "                        logger.warning(f\"Error calculating removal effect for {channel}: {str(e)}\")\n",
        "                        removal_effects[channel] = 0\n",
        "\n",
        "            # Normalize removal effects to get attribution\n",
        "            total_effect = sum(removal_effects.values())\n",
        "            if total_effect > 0:\n",
        "                attribution = {channel: effect / total_effect * 100 for channel, effect in removal_effects.items()}\n",
        "            else:\n",
        "                attribution = {channel: 0 for channel in removal_effects}\n",
        "\n",
        "            # Create results DataFrame\n",
        "            results = pd.DataFrame({\n",
        "                'touchpoint': list(attribution.keys()),\n",
        "                'attributed_conversions': [effect for effect in removal_effects.values()],\n",
        "                'attribution_percentage': [attribution[channel] for channel in attribution.keys()]\n",
        "            })\n",
        "\n",
        "            self.results['markov_chain'] = results\n",
        "            self.channel_contribution['markov_chain'] = attribution\n",
        "\n",
        "            logger.info(\"Markov Chain attribution model executed\")\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in Markov Chain attribution: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def run_shapley_value_attribution(self, max_combination_size=None):\n",
        "        \"\"\"\n",
        "        Run Shapley value attribution model based on game theory.\n",
        "\n",
        "        Args:\n",
        "            max_combination_size: Maximum size of channel combinations to consider\n",
        "                                  (None means all combinations)\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with attribution results\n",
        "        \"\"\"\n",
        "        if not self.journeys:\n",
        "            logger.error(\"No customer journeys available\")\n",
        "            return None\n",
        "\n",
        "        # Identify unique channels\n",
        "        all_channels = set()\n",
        "        for journey in self.journeys.values():\n",
        "            all_channels.update(journey['touchpoints'])\n",
        "\n",
        "        channels = list(all_channels)\n",
        "        n_channels = len(channels)\n",
        "\n",
        "        # If too many channels, limit combinations\n",
        "        if max_combination_size is None:\n",
        "            max_combination_size = n_channels\n",
        "\n",
        "        if n_channels > 10:\n",
        "            logger.warning(f\"Large number of channels ({n_channels}). This may take a long time.\")\n",
        "\n",
        "        # Create a binary feature matrix indicating which channels were involved in each journey\n",
        "        journey_data = []\n",
        "\n",
        "        for customer_id, journey in self.journeys.items():\n",
        "            journey_channels = set(journey['touchpoints'])\n",
        "            converted = journey.get('converted', False)\n",
        "\n",
        "            journey_row = {\n",
        "                'customer_id': customer_id,\n",
        "                'converted': converted\n",
        "            }\n",
        "\n",
        "            for channel in channels:\n",
        "                journey_row[f'channel_{channel}'] = 1 if channel in journey_channels else 0\n",
        "\n",
        "            journey_data.append(journey_row)\n",
        "\n",
        "        journey_df = pd.DataFrame(journey_data)\n",
        "\n",
        "        # Function to calculate conversion rate for a given set of channels\n",
        "        def conversion_rate(channel_set):\n",
        "            # Create a filter for journeys with this specific set of channels\n",
        "            query_parts = []\n",
        "            for channel in channels:\n",
        "                if channel in channel_set:\n",
        "                    query_parts.append(f\"channel_{channel} == 1\")\n",
        "                else:\n",
        "                    query_parts.append(f\"channel_{channel} == 0\")\n",
        "\n",
        "            if query_parts:\n",
        "                query = ' & '.join(query_parts)\n",
        "                filtered = journey_df.query(query)\n",
        "\n",
        "                if len(filtered) > 0:\n",
        "                    return filtered['converted'].mean()\n",
        "\n",
        "            return 0\n",
        "\n",
        "        # Calculate marginal contributions\n",
        "        shapley_values = {channel: 0 for channel in channels}\n",
        "        total_weight = 0\n",
        "\n",
        "        # Generate combinations for Shapley value calculation\n",
        "        for size in range(1, min(max_combination_size + 1, n_channels + 1)):\n",
        "            for combo in tqdm(combinations(range(n_channels), size),\n",
        "                             desc=f\"Processing combinations of size {size}\",\n",
        "                             total=scipy.special.comb(n_channels, size)):\n",
        "\n",
        "                channel_set = [channels[i] for i in combo]\n",
        "\n",
        "                # Calculate conversion rate with the current set\n",
        "                conv_rate_with = conversion_rate(channel_set)\n",
        "\n",
        "                for channel in channel_set:\n",
        "                    # Calculate conversion rate without this channel\n",
        "                    channel_set_without = [ch for ch in channel_set if ch != channel]\n",
        "                    conv_rate_without = conversion_rate(channel_set_without)\n",
        "\n",
        "                    # Marginal contribution\n",
        "                    marginal_contribution = conv_rate_with - conv_rate_without\n",
        "\n",
        "                    # Weight based on combination size\n",
        "                    weight = 1 / (scipy.special.comb(n_channels - 1, size - 1) * n_channels)\n",
        "                    shapley_values[channel] += marginal_contribution * weight\n",
        "                    total_weight += weight\n",
        "\n",
        "        # Normalize Shapley values\n",
        "        total_shapley = sum(shapley_values.values())\n",
        "        if total_shapley > 0:\n",
        "            shapley_percentages = {channel: (value / total_shapley) * 100 for channel, value in shapley_values.items()}\n",
        "        else:\n",
        "            shapley_percentages = {channel: 0 for channel in shapley_values}\n",
        "\n",
        "        # Create results DataFrame\n",
        "        results = pd.DataFrame({\n",
        "            'touchpoint': list(shapley_values.keys()),\n",
        "            'shapley_value': list(shapley_values.values()),\n",
        "            'attribution_percentage': [shapley_percentages[channel] for channel in shapley_values.keys()]\n",
        "        })\n",
        "\n",
        "        self.results['shapley'] = results\n",
        "        self.channel_contribution['shapley'] = shapley_percentages\n",
        "\n",
        "        logger.info(f\"Shapley value attribution model executed with max combination size {max_combination_size}\")\n",
        "        return results\n",
        "\n",
        "    def run_logistic_regression_attribution(self):\n",
        "        \"\"\"\n",
        "        Run logistic regression attribution model.\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with attribution results\n",
        "        \"\"\"\n",
        "        if not self.journeys:\n",
        "            logger.error(\"No customer journeys available\")\n",
        "            return None\n",
        "\n",
        "        # Identify unique channels\n",
        "        all_channels = set()\n",
        "        for journey in self.journeys.values():\n",
        "            all_channels.update(journey['touchpoints'])\n",
        "\n",
        "        channels = list(all_channels)\n",
        "\n",
        "        # Create a feature matrix for logistic regression\n",
        "        journey_data = []\n",
        "\n",
        "        for customer_id, journey in self.journeys.items():\n",
        "            journey_channels = set(journey['touchpoints'])\n",
        "            converted = journey.get('converted', False)\n",
        "\n",
        "            journey_row = {\n",
        "                'customer_id': customer_id,\n",
        "                'converted': converted\n",
        "            }\n",
        "\n",
        "            for channel in channels:\n",
        "                journey_row[f'channel_{channel}'] = 1 if channel in journey_channels else 0\n",
        "\n",
        "            journey_data.append(journey_row)\n",
        "\n",
        "        journey_df = pd.DataFrame(journey_data)\n",
        "\n",
        "        # Prepare features and target\n",
        "        X = journey_df[[f'channel_{channel}' for channel in channels]]\n",
        "        y = journey_df['converted']\n",
        "\n",
        "        # Train logistic regression model\n",
        "        model = LogisticRegression(solver='liblinear', max_iter=1000)\n",
        "\n",
        "        try:\n",
        "            model.fit(X, y)\n",
        "\n",
        "            # Extract coefficients\n",
        "            coefficients = model.coef_[0]\n",
        "\n",
        "            # Ensure coefficients are positive (for attribution)\n",
        "            coefficients = np.maximum(coefficients, 0)\n",
        "\n",
        "            # Normalize coefficients to get attribution percentages\n",
        "            total_coef = np.sum(coefficients)\n",
        "            if total_coef > 0:\n",
        "                attribution_percentages = (coefficients / total_coef) * 100\n",
        "            else:\n",
        "                attribution_percentages = np.zeros_like(coefficients)\n",
        "\n",
        "            # Create results DataFrame\n",
        "            results = pd.DataFrame({\n",
        "                'touchpoint': channels,\n",
        "                'coefficient': coefficients,\n",
        "                'attribution_percentage': attribution_percentages\n",
        "            })\n",
        "\n",
        "            self.results['logistic_regression'] = results\n",
        "            self.channel_contribution['logistic_regression'] = dict(zip(channels, attribution_percentages))\n",
        "\n",
        "            logger.info(\"Logistic regression attribution model executed\")\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in logistic regression attribution: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def run_deep_learning_attribution(self, epochs=100, batch_size=32, validation_split=0.2):\n",
        "        \"\"\"\n",
        "        Run deep learning attribution model using neural networks.\n",
        "\n",
        "        Args:\n",
        "            epochs: Number of training epochs\n",
        "            batch_size: Training batch size\n",
        "            validation_split: Fraction of data to use for validation\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with attribution results\n",
        "        \"\"\"\n",
        "        if not self.journeys:\n",
        "            logger.error(\"No customer journeys available\")\n",
        "            return None\n",
        "\n",
        "        # Identify unique channels\n",
        "        all_channels = set()\n",
        "        for journey in self.journeys.values():\n",
        "            touchpoints = journey['touchpoints']\n",
        "            all_channels.update(touchpoints)\n",
        "\n",
        "        channels = list(all_channels)\n",
        "\n",
        "        # Create feature matrices for sequences\n",
        "        sequence_data = []\n",
        "        max_seq_length = max(len(journey['touchpoints']) for journey in self.journeys.values())\n",
        "\n",
        "        for customer_id, journey in self.journeys.items():\n",
        "            touchpoints = journey['touchpoints']\n",
        "            converted = journey.get('converted', False)\n",
        "\n",
        "            # Create channel feature\n",
        "            channel_features = {channel: [] for channel in channels}\n",
        "\n",
        "            for touchpoint in touchpoints:\n",
        "                for channel in channels:\n",
        "                    channel_features[channel].append(1 if touchpoint == channel else 0)\n",
        "\n",
        "            # Pad sequences\n",
        "            for channel in channels:\n",
        "                channel_features[channel].extend([0] * (max_seq_length - len(touchpoints)))\n",
        "\n",
        "            # Add to dataset\n",
        "            row = {\n",
        "                'customer_id': customer_id,\n",
        "                'converted': converted\n",
        "            }\n",
        "            row.update({f'seq_{channel}': channel_features[channel] for channel in channels})\n",
        "\n",
        "            sequence_data.append(row)\n",
        "\n",
        "        seq_df = pd.DataFrame(sequence_data)\n",
        "\n",
        "        # Prepare features and target\n",
        "        X = np.array([seq_df[f'seq_{channel}'].tolist() for channel in channels]).transpose(1, 2, 0)\n",
        "        y = seq_df['converted'].values\n",
        "\n",
        "        # Split data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Build deep learning model with attention mechanism\n",
        "        input_layer = Input(shape=(max_seq_length, len(channels)))\n",
        "\n",
        "        # Add attention layer\n",
        "        attention = MultiHeadAttention(num_heads=2, key_dim=len(channels))(input_layer, input_layer)\n",
        "        flatten = tf.keras.layers.Flatten()(attention)\n",
        "        dense1 = Dense(64, activation='relu')(flatten)\n",
        "        dropout1 = Dropout(0.3)(dense1)\n",
        "        dense2 = Dense(32, activation='relu')(dropout1)\n",
        "        dropout2 = Dropout(0.2)(dense2)\n",
        "        output = Dense(1, activation='sigmoid')(dropout2)\n",
        "\n",
        "        model = Model(inputs=input_layer, outputs=output)\n",
        "        model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "        # Train model\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "        try:\n",
        "            history = model.fit(\n",
        "                X_train, y_train,\n",
        "                epochs=epochs,\n",
        "                batch_size=batch_size,\n",
        "                validation_split=validation_split,\n",
        "                callbacks=[early_stopping],\n",
        "                verbose=0\n",
        "            )\n",
        "\n",
        "            # Evaluate model\n",
        "            test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "            logger.info(f\"Deep learning model test accuracy: {test_acc:.4f}\")\n",
        "\n",
        "            # Feature importance analysis\n",
        "            # We'll use a perturbation-based approach for feature importance\n",
        "\n",
        "            baseline_pred = model.predict(X_test, verbose=0).mean()\n",
        "            importances = []\n",
        "\n",
        "            for i, channel in enumerate(channels):\n",
        "                # Create a copy of the test data with the channel masked\n",
        "                X_masked = X_test.copy()\n",
        "                X_masked[:, :, i] = 0\n",
        "\n",
        "                # Predict with the masked data\n",
        "                masked_pred = model.predict(X_masked, verbose=0).mean()\n",
        "\n",
        "                # Calculate importance as the difference in predictions\n",
        "                importance = baseline_pred - masked_pred\n",
        "                importances.append(max(importance, 0))  # Only consider positive contributions\n",
        "\n",
        "            # Normalize importances\n",
        "            total_importance = sum(importances)\n",
        "            if total_importance > 0:\n",
        "                attribution_percentages = [(imp / total_importance) * 100 for imp in importances]\n",
        "            else:\n",
        "                attribution_percentages = [0] * len(channels)\n",
        "\n",
        "            # Create results DataFrame\n",
        "            results = pd.DataFrame({\n",
        "                'touchpoint': channels,\n",
        "                'importance': importances,\n",
        "                'attribution_percentage': attribution_percentages\n",
        "            })\n",
        "\n",
        "            self.results['deep_learning'] = results\n",
        "            self.channel_contribution['deep_learning'] = dict(zip(channels, attribution_percentages))\n",
        "\n",
        "            logger.info(\"Deep learning attribution model executed\")\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in deep learning attribution: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def compare_models(self, models=None):\n",
        "        \"\"\"\n",
        "        Compare multiple attribution models.\n",
        "\n",
        "        Args:\n",
        "            models: List of model names to compare (None means all models)\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with comparison of attribution percentages across models\n",
        "        \"\"\"\n",
        "        if not self.results:\n",
        "            logger.warning(\"No attribution results available for comparison\")\n",
        "            return None\n",
        "\n",
        "        if models is None:\n",
        "            models = list(self.results.keys())\n",
        "        else:\n",
        "            # Filter to only include available models\n",
        "            models = [model for model in models if model in self.results]\n",
        "\n",
        "        if not models:\n",
        "            logger.warning(\"No valid models specified for comparison\")\n",
        "            return None\n",
        "\n",
        "        # Get all unique touchpoints across models\n",
        "        all_touchpoints = set()\n",
        "        for model in models:\n",
        "            touchpoints = self.results[model]['touchpoint'].tolist()\n",
        "            all_touchpoints.update(touchpoints)\n",
        "\n",
        "        # Create comparison DataFrame\n",
        "        comparison = pd.DataFrame({'touchpoint': list(all_touchpoints)})\n",
        "\n",
        "        # Add attribution percentages from each model\n",
        "        for model in models:\n",
        "            model_data = self.results[model]\n",
        "            comparison = comparison.merge(\n",
        "                model_data[['touchpoint', 'attribution_percentage']].rename(\n",
        "                    columns={'attribution_percentage': f'{model}_percentage'}\n",
        "                ),\n",
        "                on='touchpoint',\n",
        "                how='left'\n",
        "            )\n",
        "\n",
        "        # Fill NA values with 0\n",
        "        comparison.fillna(0, inplace=True)\n",
        "\n",
        "        logger.info(f\"Compared {len(models)} attribution models\")\n",
        "        return comparison\n",
        "\n",
        "\n",
        "class AdvancedAnalytics:\n",
        "    \"\"\"\n",
        "    Component for advanced analytics and causal inference techniques.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, journeys=None, data=None):\n",
        "        \"\"\"\n",
        "        Initialize the advanced analytics component.\n",
        "\n",
        "        Args:\n",
        "            journeys: Customer journeys\n",
        "            data: Raw customer data\n",
        "        \"\"\"\n",
        "        self.journeys = journeys\n",
        "        self.data = data\n",
        "        self.results = {}\n",
        "        logger.info(\"AdvancedAnalytics component initialized\")\n",
        "\n",
        "    def set_data(self, journeys=None, data=None):\n",
        "        \"\"\"\n",
        "        Set data for advanced analytics.\n",
        "\n",
        "        Args:\n",
        "            journeys: Customer journeys\n",
        "            data: Raw customer data\n",
        "        \"\"\"\n",
        "        if journeys is not None:\n",
        "            self.journeys = journeys\n",
        "        if data is not None:\n",
        "            self.data = data\n",
        "        logger.info(\"Data set for advanced analytics\")\n",
        "\n",
        "    def uplift_modeling(self, control_group_column=None):\n",
        "        \"\"\"\n",
        "        Perform uplift modeling to estimate causal impact.\n",
        "\n",
        "        Args:\n",
        "            control_group_column: Column indicating control group assignment\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with uplift analysis results\n",
        "        \"\"\"\n",
        "        if self.data is None or control_group_column is None:\n",
        "            logger.warning(\"Data or control group column not available for uplift modeling\")\n",
        "            return None\n",
        "\n",
        "        if control_group_column not in self.data.columns:\n",
        "            logger.error(f\"Control group column '{control_group_column}' not found in data\")\n",
        "            return None\n",
        "\n",
        "        # Identify touchpoint/channel column\n",
        "        channel_column = 'data_source'  # Adjust as needed\n",
        "        conversion_column = 'is_conversion'  # Adjust as needed\n",
        "\n",
        "        # Prepare data for uplift analysis\n",
        "        if channel_column not in self.data.columns or conversion_column not in self.data.columns:\n",
        "            logger.error(f\"Required columns not found in data: {channel_column}, {conversion_column}\")\n",
        "            return None\n",
        "\n",
        "        # Get unique channels\n",
        "        channels = self.data[channel_column].unique()\n",
        "\n",
        "        # Calculate uplift for each channel\n",
        "        uplift_results = []\n",
        "\n",
        "        for channel in channels:\n",
        "            # Filter data for this channel\n",
        "            channel_data = self.data[self.data[channel_column] == channel]\n",
        "\n",
        "            # Calculate conversion rates in treatment and control groups\n",
        "            treatment_conv_rate = channel_data[channel_data[control_group_column] == 0][conversion_column].mean()\n",
        "            control_conv_rate = channel_data[channel_data[control_group_column] == 1][conversion_column].mean()\n",
        "\n",
        "            # Calculate uplift\n",
        "            uplift = treatment_conv_rate - control_conv_rate\n",
        "\n",
        "            # Sample sizes\n",
        "            treatment_size = len(channel_data[channel_data[control_group_column] == 0])\n",
        "            control_size = len(channel_data[channel_data[control_group_column] == 1])\n",
        "\n",
        "            # Confidence interval using normal approximation\n",
        "            # Pooled standard error\n",
        "            p_pooled = (treatment_conv_rate * treatment_size + control_conv_rate * control_size) / (treatment_size + control_size)\n",
        "            se = np.sqrt(p_pooled * (1 - p_pooled) * (1/treatment_size + 1/control_size))\n",
        "\n",
        "            # 95% confidence interval\n",
        "            ci_lower = uplift - 1.96 * se\n",
        "            ci_upper = uplift + 1.96 * se\n",
        "\n",
        "            # Statistical significance (p-value)\n",
        "            z_score = uplift / se if se > 0 else 0\n",
        "            p_value = 2 * (1 - stats.norm.cdf(abs(z_score)))\n",
        "\n",
        "            uplift_results.append({\n",
        "                'channel': channel,\n",
        "                'treatment_conv_rate': treatment_conv_rate,\n",
        "                'control_conv_rate': control_conv_rate,\n",
        "                'uplift': uplift,\n",
        "                'relative_uplift': uplift / control_conv_rate if control_conv_rate > 0 else 0,\n",
        "                'confidence_interval_lower': ci_lower,\n",
        "                'confidence_interval_upper': ci_upper,\n",
        "                'p_value': p_value,\n",
        "                'significant': p_value < 0.05,\n",
        "                'treatment_size': treatment_size,\n",
        "                'control_size': control_size\n",
        "            })\n",
        "\n",
        "        results_df = pd.DataFrame(uplift_results)\n",
        "\n",
        "        # Calculate attribution percentages based on uplift\n",
        "        positive_uplift = results_df[results_df['uplift'] > 0]\n",
        "        total_uplift = positive_uplift['uplift'].sum()\n",
        "\n",
        "        if total_uplift > 0:\n",
        "            results_df['attribution_percentage'] = 0\n",
        "            results_df.loc[results_df['uplift'] > 0, 'attribution_percentage'] = (\n",
        "                results_df.loc[results_df['uplift'] > 0, 'uplift'] / total_uplift * 100\n",
        "            )\n",
        "        else:\n",
        "            results_df['attribution_percentage'] = 0\n",
        "\n",
        "        self.results['uplift_modeling'] = results_df\n",
        "\n",
        "        logger.info(\"Uplift modeling analysis completed\")\n",
        "        return results_df\n",
        "\n",
        "    def double_ml_attribution(self, features=None):\n",
        "        \"\"\"\n",
        "        Perform double machine learning for causal inference.\n",
        "\n",
        "        Args:\n",
        "            features: List of feature columns to use\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with double ML attribution results\n",
        "        \"\"\"\n",
        "        if self.data is None:\n",
        "            logger.warning(\"Data not available for double ML attribution\")\n",
        "            return None\n",
        "\n",
        "        # Identify touchpoint/channel column\n",
        "        channel_column = 'data_source'  # Adjust as needed\n",
        "        conversion_column = 'is_conversion'  # Adjust as needed\n",
        "\n",
        "        if channel_column not in self.data.columns or conversion_column not in self.data.columns:\n",
        "            logger.error(f\"Required columns not found in data: {channel_column}, {conversion_column}\")\n",
        "            return None\n",
        "\n",
        "        # Prepare features if not specified\n",
        "        if features is None:\n",
        "            # Use all columns except channel and conversion\n",
        "            features = [col for col in self.data.columns if col not in [channel_column, conversion_column]]\n",
        "\n",
        "        # Get unique channels\n",
        "        channels = self.data[channel_column].unique()\n",
        "\n",
        "        # Prepare feature data\n",
        "        X = self.data[features]\n",
        "        y = self.data[conversion_column]\n",
        "\n",
        "        # Double ML results\n",
        "        double_ml_results = []\n",
        "\n",
        "        for channel in channels:\n",
        "            # Treatment indicator for this channel\n",
        "            T = (self.data[channel_column] == channel).astype(int)\n",
        "\n",
        "            try:\n",
        "                # First stage: predict treatment using features\n",
        "                treatment_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "                treatment_model.fit(X, T)\n",
        "                T_pred = treatment_model.predict_proba(X)[:, 1]\n",
        "\n",
        "                # Treatment residuals\n",
        "                T_resid = T - T_pred\n",
        "\n",
        "                # Second stage: predict outcome using features\n",
        "                outcome_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "                outcome_model.fit(X, y)\n",
        "                y_pred = outcome_model.predict_proba(X)[:, 1]\n",
        "\n",
        "                # Outcome residuals\n",
        "                y_resid = y - y_pred\n",
        "\n",
        "                # Final stage: regress outcome residuals on treatment residuals\n",
        "                final_model = LinearRegression()\n",
        "                final_model.fit(T_resid.reshape(-1, 1), y_resid)\n",
        "\n",
        "                # Coefficient represents the causal effect\n",
        "                causal_effect = final_model.coef_[0]\n",
        "\n",
        "                # Standard error\n",
        "                n = len(T_resid)\n",
        "                se = np.sqrt(np.sum((y_resid - final_model.predict(T_resid.reshape(-1, 1)))**2) / (n - 1)) / np.sqrt(np.sum(T_resid**2))\n",
        "\n",
        "                # 95% confidence interval\n",
        "                ci_lower = causal_effect - 1.96 * se\n",
        "                ci_upper = causal_effect + 1.96 * se\n",
        "\n",
        "                # p-value\n",
        "                t_stat = causal_effect / se if se > 0 else 0\n",
        "                p_value = 2 * (1 - stats.t.cdf(abs(t_stat), df=n-1))\n",
        "\n",
        "                double_ml_results.append({\n",
        "                    'channel': channel,\n",
        "                    'causal_effect': causal_effect,\n",
        "                    'standard_error': se,\n",
        "                    'confidence_interval_lower': ci_lower,\n",
        "                    'confidence_interval_upper': ci_upper,\n",
        "                    'p_value': p_value,\n",
        "                    'significant': p_value < 0.05\n",
        "                })\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error in double ML for channel {channel}: {str(e)}\")\n",
        "                double_ml_results.append({\n",
        "                    'channel': channel,\n",
        "                    'causal_effect': np.nan,\n",
        "                    'standard_error': np.nan,\n",
        "                    'confidence_interval_lower': np.nan,\n",
        "                    'confidence_interval_upper': np.nan,\n",
        "                    'p_value': np.nan,\n",
        "                    'significant': False\n",
        "                })\n",
        "\n",
        "        results_df = pd.DataFrame(double_ml_results)\n",
        "\n",
        "        # Calculate attribution percentages based on positive causal effects\n",
        "        positive_effects = results_df[results_df['causal_effect'] > 0]\n",
        "        total_effect = positive_effects['causal_effect'].sum()\n",
        "\n",
        "        if total_effect > 0:\n",
        "            results_df['attribution_percentage'] = 0\n",
        "            results_df.loc[results_df['causal_effect'] > 0, 'attribution_percentage'] = (\n",
        "                results_df.loc[results_df['causal_effect'] > 0, 'causal_effect'] / total_effect * 100\n",
        "            )\n",
        "        else:\n",
        "            results_df['attribution_percentage'] = 0\n",
        "\n",
        "        self.results['double_ml'] = results_df\n",
        "\n",
        "        logger.info(\"Double ML attribution analysis completed\")\n",
        "        return results_df\n",
        "\n",
        "    def reinforcement_learning_allocation(self, budget=1000, channels=None, returns=None, volatility=None, n_simulations=1000):\n",
        "        \"\"\"\n",
        "        Use reinforcement learning principles to allocate marketing budget.\n",
        "\n",
        "        Args:\n",
        "            budget: Total marketing budget to allocate\n",
        "            channels: List of marketing channels\n",
        "            returns: Dict mapping channels to expected returns (ROI)\n",
        "            volatility: Dict mapping channels to volatility (risk)\n",
        "            n_simulations: Number of Monte Carlo simulations\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with budget allocation recommendations\n",
        "        \"\"\"\n",
        "        if channels is None and self.journeys is not None:\n",
        "            # Extract channels from journeys\n",
        "            all_channels = set()\n",
        "            for journey in self.journeys.values():\n",
        "                all_channels.update(journey['touchpoints'])\n",
        "            channels = list(all_channels)\n",
        "\n",
        "        if not channels:\n",
        "            logger.error(\"No channels available for budget allocation\")\n",
        "            return None\n",
        "\n",
        "        # Default returns and volatility if not provided\n",
        "        if returns is None:\n",
        "            returns = {channel: 1.0 for channel in channels}\n",
        "\n",
        "        if volatility is None:\n",
        "            volatility = {channel: 0.5 for channel in channels}\n",
        "\n",
        "        # Thompson sampling for budget allocation\n",
        "        allocations = []\n",
        "\n",
        "        for _ in range(n_simulations):\n",
        "            # Sample expected returns based on posterior distributions\n",
        "            sampled_returns = {\n",
        "                channel: np.random.normal(returns[channel], volatility[channel])\n",
        "                for channel in channels\n",
        "            }\n",
        "\n",
        "            # For this simulation, allocate all budget to the channel with highest sampled return\n",
        "            best_channel = max(sampled_returns, key=sampled_returns.get)\n",
        "\n",
        "            # Record allocation\n",
        "            allocation = {channel: 0 for channel in channels}\n",
        "            allocation[best_channel] = budget\n",
        "            allocations.append(allocation)\n",
        "\n",
        "        # Aggregate results across simulations\n",
        "        final_allocation = {channel: 0 for channel in channels}\n",
        "        for allocation in allocations:\n",
        "            for channel, amount in allocation.items():\n",
        "                final_allocation[channel] += amount / n_simulations\n",
        "\n",
        "        # Create results DataFrame\n",
        "        results = pd.DataFrame({\n",
        "            'channel': list(final_allocation.keys()),\n",
        "            'allocation': list(final_allocation.values()),\n",
        "            'allocation_percentage': [alloc / budget * 100 for alloc in final_allocation.values()],\n",
        "            'expected_return': [returns[channel] for channel in final_allocation.keys()],\n",
        "            'volatility': [volatility[channel] for channel in final_allocation.keys()]\n",
        "        })\n",
        "\n",
        "        self.results['rl_allocation'] = results\n",
        "\n",
        "        logger.info(\"Reinforcement learning budget allocation completed\")\n",
        "        return results\n",
        "\n",
        "    def run_network_analysis(self):\n",
        "        \"\"\"\n",
        "        Perform network analysis on customer journey touchpoints.\n",
        "\n",
        "        Returns:\n",
        "            Graph object and DataFrame with network metrics\n",
        "        \"\"\"\n",
        "        if not self.journeys:\n",
        "            logger.warning(\"No customer journeys available for network analysis\")\n",
        "            return None\n",
        "\n",
        "        # Create a directed graph\n",
        "        G = nx.DiGraph()\n",
        "\n",
        "        # Add nodes for all touchpoints\n",
        "        all_touchpoints = set()\n",
        "        for journey in self.journeys.values():\n",
        "            all_touchpoints.update(journey['touchpoints'])\n",
        "\n",
        "        for touchpoint in all_touchpoints:\n",
        "            G.add_node(touchpoint)\n",
        "\n",
        "        # Add edges between consecutive touchpoints in journeys\n",
        "        for customer_id, journey in self.journeys.items():\n",
        "            touchpoints = journey['touchpoints']\n",
        "            converted = journey.get('converted', False)\n",
        "\n",
        "            for i in range(len(touchpoints) - 1):\n",
        "                from_touch = touchpoints[i]\n",
        "                to_touch = touchpoints[i + 1]\n",
        "\n",
        "                # Add or increment edge weight\n",
        "                if G.has_edge(from_touch, to_touch):\n",
        "                    G[from_touch][to_touch]['weight'] += 1\n",
        "                    G[from_touch][to_touch]['conversions'] = G[from_touch][to_touch].get('conversions', 0) + converted\n",
        "                else:\n",
        "                    G.add_edge(from_touch, to_touch, weight=1, conversions=int(converted))\n",
        "\n",
        "        # Calculate network metrics\n",
        "        metrics = {}\n",
        "\n",
        "        # Degree centrality (number of connections)\n",
        "        in_degree = dict(G.in_degree(weight='weight'))\n",
        "        out_degree = dict(G.out_degree(weight='weight'))\n",
        "\n",
        "        # Closeness centrality (how close a node is to all other nodes)\n",
        "        try:\n",
        "            closeness = nx.closeness_centrality(G, distance='weight')\n",
        "        except:\n",
        "            closeness = {node: 0 for node in G.nodes()}\n",
        "\n",
        "        # Betweenness centrality (how often a node is in the shortest path between other nodes)\n",
        "        try:\n",
        "            betweenness = nx.betweenness_centrality(G, weight='weight')\n",
        "        except:\n",
        "            betweenness = {node: 0 for node in G.nodes()}\n",
        "\n",
        "        # PageRank (importance of node based on connections)\n",
        "        pagerank = nx.pagerank(G, weight='weight')\n",
        "\n",
        "        # Compile metrics\n",
        "        for node in G.nodes():\n",
        "            metrics[node] = {\n",
        "                'touchpoint': node,\n",
        "                'in_degree': in_degree.get(node, 0),\n",
        "                'out_degree': out_degree.get(node, 0),\n",
        "                'closeness_centrality': closeness.get(node, 0),\n",
        "                'betweenness_centrality': betweenness.get(node, 0),\n",
        "                'pagerank': pagerank.get(node, 0)\n",
        "            }\n",
        "\n",
        "        # Create DataFrame from metrics\n",
        "        metrics_df = pd.DataFrame(list(metrics.values()))\n",
        "\n",
        "        # Calculate conversion rates for edges\n",
        "        edge_metrics = []\n",
        "        for u, v, data in G.edges(data=True):\n",
        "            if 'weight' in data and 'conversions' in data:\n",
        "                conversion_rate = data['conversions'] / data['weight']\n",
        "                edge_metrics.append({\n",
        "                    'from_touchpoint': u,\n",
        "                    'to_touchpoint': v,\n",
        "                    'frequency': data['weight'],\n",
        "                    'conversions': data['conversions'],\n",
        "                    'conversion_rate': conversion_rate\n",
        "                })\n",
        "\n",
        "        edge_df = pd.DataFrame(edge_metrics)\n",
        "\n",
        "        # Store results\n",
        "        self.results['network_analysis'] = {\n",
        "            'graph': G,\n",
        "            'node_metrics': metrics_df,\n",
        "            'edge_metrics': edge_df\n",
        "        }\n",
        "\n",
        "        logger.info(\"Network analysis completed\")\n",
        "        return self.results['network_analysis']\n",
        "\n",
        "    def customer_segmentation(self, n_clusters=5):\n",
        "        \"\"\"\n",
        "        Perform customer segmentation based on journey patterns.\n",
        "\n",
        "        Args:\n",
        "            n_clusters: Number of customer segments to create\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with customer segments and their characteristics\n",
        "        \"\"\"\n",
        "        if not self.journeys:\n",
        "            logger.warning(\"No customer journeys available for segmentation\")\n",
        "            return None\n",
        "\n",
        "        # Get unique channels/touchpoints\n",
        "        all_touchpoints = set()\n",
        "        for journey in self.journeys.values():\n",
        "            all_touchpoints.update(journey['touchpoints'])\n",
        "\n",
        "        channels = list(all_touchpoints)\n",
        "\n",
        "        # Create feature matrix for clustering\n",
        "        customer_features = []\n",
        "\n",
        "        for customer_id, journey in self.journeys.items():\n",
        "            touchpoints = journey['touchpoints']\n",
        "            touchpoint_counts = {touchpoint: touchpoints.count(touchpoint) for touchpoint in set(touchpoints)}\n",
        "\n",
        "            features = {\n",
        "                'customer_id': customer_id,\n",
        "                'journey_length': len(touchpoints),\n",
        "                'unique_touchpoints': len(set(touchpoints)),\n",
        "                'converted': journey.get('converted', False)\n",
        "            }\n",
        "\n",
        "            # Add touchpoint counts\n",
        "            for channel in channels:\n",
        "                features[f'count_{channel}'] = touchpoint_counts.get(channel, 0)\n",
        "\n",
        "            customer_features.append(features)\n",
        "\n",
        "        df_features = pd.DataFrame(customer_features)\n",
        "\n",
        "        # Prepare data for clustering\n",
        "        feature_cols = ['journey_length', 'unique_touchpoints'] + [f'count_{channel}' for channel in channels]\n",
        "        X = df_features[feature_cols].values\n",
        "\n",
        "        # Standardize features\n",
        "        scaler = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "        # Perform clustering (KMeans)\n",
        "        from sklearn.cluster import KMeans\n",
        "\n",
        "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "        df_features['cluster'] = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "        # Analyze clusters\n",
        "        cluster_stats = df_features.groupby('cluster').agg({\n",
        "            'customer_id': 'count',\n",
        "            'journey_length': 'mean',\n",
        "            'unique_touchpoints': 'mean',\n",
        "            'converted': 'mean'\n",
        "        }).reset_index()\n",
        "\n",
        "        cluster_stats.rename(columns={\n",
        "            'customer_id': 'customer_count',\n",
        "            'converted': 'conversion_rate'\n",
        "        }, inplace=True)\n",
        "\n",
        "        # Touchpoint distribution by cluster\n",
        "        touchpoint_cols = [f'count_{channel}' for channel in channels]\n",
        "        touchpoint_means = df_features.groupby('cluster')[touchpoint_cols].mean().reset_index()\n",
        "\n",
        "        # Merge with cluster stats\n",
        "        cluster_stats = cluster_stats.merge(touchpoint_means, on='cluster')\n",
        "\n",
        "        # Determine defining characteristics for each cluster\n",
        "        cluster_profiles = {}\n",
        "\n",
        "        for cluster in range(n_clusters):\n",
        "            cluster_data = cluster_stats[cluster_stats['cluster'] == cluster]\n",
        "\n",
        "            # Basic info\n",
        "            profile = {\n",
        "                'cluster': cluster,\n",
        "                'size': cluster_data['customer_count'].values[0],\n",
        "                'conversion_rate': cluster_data['conversion_rate'].values[0],\n",
        "                'avg_journey_length': cluster_data['journey_length'].values[0],\n",
        "                'avg_unique_touchpoints': cluster_data['unique_touchpoints'].values[0]\n",
        "            }\n",
        "\n",
        "            # Top touchpoints\n",
        "            touchpoint_data = {}\n",
        "            for channel in channels:\n",
        "                col = f'count_{channel}'\n",
        "                if col in cluster_data.columns:\n",
        "                    touchpoint_data[channel] = cluster_data[col].values[0]\n",
        "\n",
        "            # Get top 3 touchpoints\n",
        "            top_touchpoints = sorted(touchpoint_data.items(), key=lambda x: x[1], reverse=True)[:3]\n",
        "            profile['top_touchpoints'] = [t[0] for t in top_touchpoints if t[1] > 0]\n",
        "\n",
        "            cluster_profiles[cluster] = profile\n",
        "\n",
        "        # Create profiles DataFrame\n",
        "        profiles_df = pd.DataFrame(list(cluster_profiles.values()))\n",
        "\n",
        "        # Store results\n",
        "        self.results['customer_segments'] = {\n",
        "            'clusters': df_features,\n",
        "            'cluster_stats': cluster_stats,\n",
        "            'profiles': profiles_df\n",
        "        }\n",
        "\n",
        "        logger.info(f\"Customer segmentation completed with {n_clusters} clusters\")\n",
        "        return self.results['customer_segments']\n",
        "\n",
        "\n",
        "class Visualization:\n",
        "    \"\"\"\n",
        "    Component for creating visualizations of attribution results and customer journeys.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, attribution_models=None, customer_journey=None, advanced_analytics=None):\n",
        "        \"\"\"\n",
        "        Initialize the visualization component.\n",
        "\n",
        "        Args:\n",
        "            attribution_models: AttributionModels component\n",
        "            customer_journey: CustomerJourney component\n",
        "            advanced_analytics: AdvancedAnalytics component\n",
        "        \"\"\"\n",
        "        self.attribution_models = attribution_models\n",
        "        self.customer_journey = customer_journey\n",
        "        self.advanced_analytics = advanced_analytics\n",
        "        logger.info(\"Visualization component initialized\")\n",
        "\n",
        "    def set_components(self, attribution_models=None, customer_journey=None, advanced_analytics=None):\n",
        "        \"\"\"\n",
        "        Set components for visualization.\n",
        "\n",
        "        Args:\n",
        "            attribution_models: AttributionModels component\n",
        "            customer_journey: CustomerJourney component\n",
        "            advanced_analytics: AdvancedAnalytics component\n",
        "        \"\"\"\n",
        "        if attribution_models is not None:\n",
        "            self.attribution_models = attribution_models\n",
        "        if customer_journey is not None:\n",
        "            self.customer_journey = customer_journey\n",
        "        if advanced_analytics is not None:\n",
        "            self.advanced_analytics = advanced_analytics\n",
        "        logger.info(\"Components set for visualization\")\n",
        "\n",
        "    def plot_attribution_comparison(self, models=None, save_path=None):\n",
        "        \"\"\"\n",
        "        Create a bar chart comparing attribution percentages across models.\n",
        "\n",
        "        Args:\n",
        "            models: List of model names to compare (None means all models)\n",
        "            save_path: Path to save the visualization\n",
        "\n",
        "        Returns:\n",
        "            Matplotlib figure\n",
        "        \"\"\"\n",
        "        if self.attribution_models is None or not self.attribution_models.results:\n",
        "            logger.warning(\"No attribution results available for visualization\")\n",
        "            return None\n",
        "\n",
        "        # Get comparison data\n",
        "        comparison = self.attribution_models.compare_models(models)\n",
        "\n",
        "        if comparison is None:\n",
        "            return None\n",
        "\n",
        "        # Prepare data for plotting\n",
        "        model_cols = [col for col in comparison.columns if col.endswith('_percentage')]\n",
        "        models = [col.replace('_percentage', '') for col in model_cols]\n",
        "\n",
        "        # Set up the figure\n",
        "        fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "        # Generate colors\n",
        "        colors = plt.cm.tab20(np.linspace(0, 1, len(model_cols)))\n",
        "\n",
        "        # Sort touchpoints by average attribution\n",
        "        comparison['avg_attribution'] = comparison[model_cols].mean(axis=1)\n",
        "        comparison = comparison.sort_values('avg_attribution', ascending=False)\n",
        "\n",
        "        # Plot bars\n",
        "        bar_width = 0.8 / len(model_cols)\n",
        "        touchpoints = comparison['touchpoint'].tolist()\n",
        "\n",
        "        for i, (col, color) in enumerate(zip(model_cols, colors)):\n",
        "            position = np.arange(len(touchpoints)) + i * bar_width - 0.4 + bar_width/2\n",
        "            ax.bar(position, comparison[col], width=bar_width, color=color,\n",
        "                   label=col.replace('_percentage', ''))\n",
        "\n",
        "        # Set labels and title\n",
        "        ax.set_title('Comparison of Attribution Models', fontsize=16)\n",
        "        ax.set_xlabel('Touchpoint', fontsize=12)\n",
        "        ax.set_ylabel('Attribution Percentage (%)', fontsize=12)\n",
        "        ax.set_xticks(np.arange(len(touchpoints)))\n",
        "        ax.set_xticklabels(touchpoints, rotation=45, ha='right')\n",
        "        ax.legend(title='Attribution Model')\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        if save_path:\n",
        "            plt.savefig(save_path)\n",
        "            logger.info(f\"Attribution comparison saved to {save_path}\")\n",
        "\n",
        "        return fig\n",
        "\n",
        "    def plot_customer_journey_sankey(self, top_n=50, save_path=None):\n",
        "        \"\"\"\n",
        "        Create a Sankey diagram of customer journeys.\n",
        "\n",
        "        Args:\n",
        "            top_n: Number of top journeys to include\n",
        "            save_path: Path to save the visualization\n",
        "\n",
        "        Returns:\n",
        "            Plotly figure\n",
        "        \"\"\"\n",
        "        if self.customer_journey is None or self.customer_journey.touchpoint_sequence is None:\n",
        "            logger.warning(\"No customer journey data available for visualization\")\n",
        "            return None\n",
        "\n",
        "        # Get top journeys\n",
        "        journey_counts = self.customer_journey.touchpoint_sequence['sequence'].value_counts().reset_index()\n",
        "        journey_counts.columns = ['journey', 'count']\n",
        "        journey_counts = journey_counts.head(top_n)\n",
        "\n",
        "        # Process journey sequences\n",
        "        links = []\n",
        "\n",
        "        for _, row in journey_counts.iterrows():\n",
        "            journey = row['journey']\n",
        "            count = row['count']\n",
        "            touchpoints = journey.split('->')\n",
        "\n",
        "            for i in range(len(touchpoints) - 1):\n",
        "                links.append({\n",
        "                    'source': touchpoints[i],\n",
        "                    'target': touchpoints[i + 1],\n",
        "                    'value': count\n",
        "                })\n",
        "\n",
        "        # Aggregate links\n",
        "        links_df = pd.DataFrame(links)\n",
        "        agg_links = links_df.groupby(['source', 'target']).sum().reset_index()\n",
        "\n",
        "        # Create Sankey diagram\n",
        "        all_nodes = set(agg_links['source']).union(set(agg_links['target']))\n",
        "        node_mapping = {node: i for i, node in enumerate(all_nodes)}\n",
        "\n",
        "        # Create Sankey data\n",
        "        sankey_data = {\n",
        "            'node': {\n",
        "                'label': list(all_nodes),\n",
        "                'pad': 15,\n",
        "                'thickness': 20\n",
        "            },\n",
        "            'link': {\n",
        "                'source': [node_mapping[row['source']] for _, row in agg_links.iterrows()],\n",
        "                'target': [node_mapping[row['target']] for _, row in agg_links.iterrows()],\n",
        "                'value': agg_links['value'].tolist()\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Create figure\n",
        "        fig = go.Figure(data=[go.Sankey(\n",
        "            arrangement='snap',\n",
        "            node=sankey_data['node'],\n",
        "            link=sankey_data['link']\n",
        "        )])\n",
        "\n",
        "        fig.update_layout(\n",
        "            title='Customer Journey Flow',\n",
        "            font=dict(size=10),\n",
        "            margin=dict(l=0, r=0, t=40, b=0)\n",
        "        )\n",
        "\n",
        "        if save_path:\n",
        "            fig.write_html(save_path)\n",
        "            logger.info(f\"Customer journey Sankey diagram saved to {save_path}\")\n",
        "\n",
        "        return fig\n",
        "\n",
        "    def plot_conversion_funnel(self, save_path=None):\n",
        "        \"\"\"\n",
        "        Create a funnel chart of conversion rates by funnel stage.\n",
        "\n",
        "        Args:\n",
        "            save_path: Path to save the visualization\n",
        "\n",
        "        Returns:\n",
        "            Plotly figure\n",
        "        \"\"\"\n",
        "        if self.customer_journey is None or self.customer_journey.data is None or 'funnel_stage' not in self.customer_journey.data.columns:\n",
        "            logger.warning(\"No funnel stage data available for visualization\")\n",
        "            return None\n",
        "\n",
        "        # Group by funnel stage and calculate metrics\n",
        "        funnel_data = self.customer_journey.data.groupby('funnel_stage').agg({\n",
        "            'customer_id': pd.Series.nunique,\n",
        "            'is_conversion': 'sum'\n",
        "        }).reset_index()\n",
        "\n",
        "        funnel_data.columns = ['funnel_stage', 'customers', 'conversions']\n",
        "        funnel_data['conversion_rate'] = funnel_data['conversions'] / funnel_data['customers'] * 100\n",
        "\n",
        "        # Order stages logically (if possible)\n",
        "        stage_order = ['awareness', 'consideration', 'intent', 'purchase', 'retention']\n",
        "        available_stages = [stage for stage in stage_order if stage in funnel_data['funnel_stage'].values]\n",
        "\n",
        "        if available_stages:\n",
        "            funnel_data = funnel_data[funnel_data['funnel_stage'].isin(available_stages)]\n",
        "            funnel_data['stage_order'] = funnel_data['funnel_stage'].map({stage: i for i, stage in enumerate(stage_order)})\n",
        "            funnel_data = funnel_data.sort_values('stage_order')\n",
        "\n",
        "        # Create funnel chart\n",
        "        fig = go.Figure(go.Funnel(\n",
        "            y=funnel_data['funnel_stage'],\n",
        "            x=funnel_data['customers'],\n",
        "            textinfo='value+percent initial',\n",
        "            marker=dict(color=px.colors.sequential.Blues),\n",
        "            connector=dict(line=dict(width=1))\n",
        "        ))\n",
        "\n",
        "        fig.update_layout(\n",
        "            title='Customer Conversion Funnel',\n",
        "            margin=dict(l=150, r=0, t=40, b=0)\n",
        "        )\n",
        "\n",
        "        if save_path:\n",
        "            fig.write_html(save_path)\n",
        "            logger.info(f\"Conversion funnel chart saved to {save_path}\")\n",
        "\n",
        "        return fig\n",
        "\n",
        "    def plot_network_graph(self, save_path=None):\n",
        "        \"\"\"\n",
        "        Create a network graph visualization of touchpoint interactions.\n",
        "\n",
        "        Args:\n",
        "            save_path: Path to save the visualization\n",
        "\n",
        "        Returns:\n",
        "            Matplotlib figure\n",
        "        \"\"\"\n",
        "        if self.advanced_analytics is None or 'network_analysis' not in self.advanced_analytics.results:\n",
        "            logger.warning(\"No network analysis results available for visualization\")\n",
        "            return None\n",
        "\n",
        "        # Get network graph\n",
        "        G = self.advanced_analytics.results['network_analysis']['graph']\n",
        "\n",
        "        # Create figure\n",
        "        plt.figure(figsize=(12, 10))\n",
        "\n",
        "        # Calculate node positions using spring layout\n",
        "        pos = nx.spring_layout(G, k=0.3, iterations=50)\n",
        "\n",
        "        # Calculate node sizes based on pagerank\n",
        "        pagerank = nx.pagerank(G, weight='weight')\n",
        "        node_sizes = [pagerank[node] * 5000 for node in G.nodes()]\n",
        "\n",
        "        # Calculate edge widths based on weight\n",
        "        edge_widths = [G[u][v].get('weight', 1) / 10 for u, v in G.edges()]\n",
        "\n",
        "        # Draw network\n",
        "        nx.draw_networkx_nodes(G, pos, node_size=node_sizes,\n",
        "                             node_color=list(pagerank.values()),\n",
        "                             cmap=plt.cm.Blues, alpha=0.8)\n",
        "\n",
        "        nx.draw_networkx_edges(G, pos, width=edge_widths,\n",
        "                             edge_color='lightgray', arrows=True,\n",
        "                             arrowsize=15, arrowstyle='->')\n",
        "\n",
        "        nx.draw_networkx_labels(G, pos, font_size=10, font_family='sans-serif')\n",
        "\n",
        "        plt.title('Touchpoint Interaction Network', fontsize=16)\n",
        "        plt.axis('off')\n",
        "\n",
        "        if save_path:\n",
        "            plt.savefig(save_path, bbox_inches='tight')\n",
        "            logger.info(f\"Network graph saved to {save_path}\")\n",
        "\n",
        "        return plt.gcf()\n",
        "\n",
        "    def plot_shap_importance(self, model_name='logistic_regression', save_path=None):\n",
        "        \"\"\"\n",
        "        Create a SHAP summary plot for model feature importance.\n",
        "\n",
        "        Args:\n",
        "            model_name: Name of the attribution model\n",
        "            save_path: Path to save the visualization\n",
        "\n",
        "        Returns:\n",
        "            Matplotlib figure\n",
        "        \"\"\"\n",
        "        if self.attribution_models is None or model_name not in self.attribution_models.results:\n",
        "            logger.warning(f\"No {model_name} results available for SHAP visualization\")\n",
        "            return None\n",
        "\n",
        "        # Get model results\n",
        "        model_results = self.attribution_models.results[model_name]\n",
        "\n",
        "        # Create a simple bar chart of attribution percentages\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "        # Sort by attribution percentage\n",
        "        model_results = model_results.sort_values('attribution_percentage', ascending=False)\n",
        "\n",
        "        # Plot bars\n",
        "        bars = ax.barh(model_results['touchpoint'], model_results['attribution_percentage'],\n",
        "                      color=plt.cm.Blues(np.linspace(0.3, 0.8, len(model_results))))\n",
        "\n",
        "        # Add values on bars\n",
        "        for bar in bars:\n",
        "            width = bar.get_width()\n",
        "            ax.text(width + 1, bar.get_y() + bar.get_height()/2,\n",
        "                   f'{width:.1f}%', ha='left', va='center')\n",
        "\n",
        "        # Set labels and title\n",
        "        ax.set_title(f'{model_name.replace(\"_\", \" \").title()} Attribution', fontsize=16)\n",
        "        ax.set_xlabel('Attribution Percentage (%)', fontsize=12)\n",
        "        ax.set_ylabel('Touchpoint', fontsize=12)\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        if save_path:\n",
        "            plt.savefig(save_path)\n",
        "            logger.info(f\"SHAP importance plot saved to {save_path}\")\n",
        "\n",
        "        return fig\n",
        "\n",
        "    def plot_customer_segments(self, save_path=None):\n",
        "        \"\"\"\n",
        "        Create a visualization of customer segments.\n",
        "\n",
        "        Args:\n",
        "            save_path: Path to save the visualization\n",
        "\n",
        "        Returns:\n",
        "            Matplotlib figure\n",
        "        \"\"\"\n",
        "        if self.advanced_analytics is None or 'customer_segments' not in self.advanced_analytics.results:\n",
        "            logger.warning(\"No customer segmentation results available for visualization\")\n",
        "            return None\n",
        "\n",
        "        # Get segmentation results\n",
        "        segments = self.advanced_analytics.results['customer_segments']['clusters']\n",
        "        profiles = self.advanced_analytics.results['customer_segments']['profiles']\n",
        "\n",
        "        # Create scatter plot with dimensions reduction\n",
        "        from sklearn.decomposition import PCA\n",
        "\n",
        "        # Prepare data for PCA\n",
        "        feature_cols = [col for col in segments.columns if col.startswith('count_')\n",
        "                      or col in ['journey_length', 'unique_touchpoints']]\n",
        "        X = segments[feature_cols].values\n",
        "\n",
        "        # Standardize features\n",
        "        scaler = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "        # Apply PCA for 2D visualization\n",
        "        pca = PCA(n_components=2)\n",
        "        X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "        # Create figure\n",
        "        fig, ax = plt.subplots(figsize=(12, 10))\n",
        "\n",
        "        # Plot points colored by cluster\n",
        "        clusters = segments['cluster'].unique()\n",
        "        colors = plt.cm.tab10(np.linspace(0, 1, len(clusters)))\n",
        "\n",
        "        for i, cluster in enumerate(clusters):\n",
        "            mask = segments['cluster'] == cluster\n",
        "            ax.scatter(X_pca[mask, 0], X_pca[mask, 1],\n",
        "                      c=[colors[i]], label=f'Cluster {cluster}',\n",
        "                      alpha=0.7, s=50)\n",
        "\n",
        "        # Add cluster information\n",
        "        for i, row in profiles.iterrows():\n",
        "            cluster = row['cluster']\n",
        "            cluster_points = X_pca[segments['cluster'] == cluster]\n",
        "\n",
        "            # Calculate centroid\n",
        "            centroid_x = cluster_points[:, 0].mean()\n",
        "            centroid_y = cluster_points[:, 1].mean()\n",
        "\n",
        "            # Add label\n",
        "            top_touchpoints = ', '.join(row['top_touchpoints'][:2])\n",
        "            label = f\"Cluster {cluster}\\nSize: {row['size']}\\nConv: {row['conversion_rate']:.1%}\\n{top_touchpoints}\"\n",
        "\n",
        "            ax.text(centroid_x, centroid_y, label,\n",
        "                   fontsize=9, ha='center', va='center',\n",
        "                   bbox=dict(facecolor='white', alpha=0.5))\n",
        "\n",
        "        # Set labels and title\n",
        "        explained_var = pca.explained_variance_ratio_\n",
        "        ax.set_xlabel(f'Component 1 ({explained_var[0]:.1%} variance)', fontsize=12)\n",
        "        ax.set_ylabel(f'Component 2 ({explained_var[1]:.1%} variance)', fontsize=12)\n",
        "        ax.set_title('Customer Segments Visualization', fontsize=16)\n",
        "\n",
        "        # Add legend\n",
        "        ax.legend(title='Segments')\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        if save_path:\n",
        "            plt.savefig(save_path)\n",
        "            logger.info(f\"Customer segments visualization saved to {save_path}\")\n",
        "\n",
        "        return fig\n",
        "\n",
        "    def create_interactive_dashboard(self, save_path=None):\n",
        "        \"\"\"\n",
        "        Create an interactive dashboard of attribution results.\n",
        "\n",
        "        Args:\n",
        "            save_path: Path to save the dashboard\n",
        "\n",
        "        Returns:\n",
        "            Plotly figure\n",
        "        \"\"\"\n",
        "        if self.attribution_models is None or not self.attribution_models.results:\n",
        "            logger.warning(\"No attribution results available for dashboard\")\n",
        "            return None\n",
        "\n",
        "        # Get comparison data\n",
        "        comparison = self.attribution_models.compare_models()\n",
        "\n",
        "        if comparison is None:\n",
        "            return None\n",
        "\n",
        "        # Create subplot figure\n",
        "        fig = make_subplots(\n",
        "            rows=2, cols=2,\n",
        "            subplot_titles=('Attribution Model Comparison', 'Top Channels by Model',\n",
        "                           'Conversion Rates by Channel', 'Attribution Over Time'),\n",
        "            vertical_spacing=0.1\n",
        "        )\n",
        "\n",
        "        # 1. Model comparison chart (top-left)\n",
        "        model_cols = [col for col in comparison.columns if col.endswith('_percentage')]\n",
        "        models = [col.replace('_percentage', '') for col in model_cols]\n",
        "\n",
        "        # Sort touchpoints by average attribution\n",
        "        comparison['avg_attribution'] = comparison[model_cols].mean(axis=1)\n",
        "        comparison = comparison.sort_values('avg_attribution', ascending=False).head(10)\n",
        "\n",
        "        # Add traces for each model\n",
        "        for i, model in enumerate(models):\n",
        "            col = f'{model}_percentage'\n",
        "            fig.add_trace(\n",
        "                go.Bar(\n",
        "                    x=comparison['touchpoint'],\n",
        "                    y=comparison[col],\n",
        "                    name=model,\n",
        "                    marker_color=px.colors.qualitative.Plotly[i % 10]\n",
        "                ),\n",
        "                row=1, col=1\n",
        "            )\n",
        "\n",
        "        # 2. Top channels by model (top-right)\n",
        "        # Get top channel for each model\n",
        "        top_channels = pd.DataFrame({\n",
        "            'model': models,\n",
        "            'top_channel': [comparison.iloc[comparison[f'{model}_percentage'].argmax()]['touchpoint'] for model in models],\n",
        "            'attribution': [comparison[f'{model}_percentage'].max() for model in models]\n",
        "        })\n",
        "\n",
        "        fig.add_trace(\n",
        "            go.Bar(\n",
        "                x=top_channels['model'],\n",
        "                y=top_channels['attribution'],\n",
        "                text=top_channels['top_channel'],\n",
        "                textposition='auto',\n",
        "                hovertemplate='Model: %{x}<br>Top Channel: %{text}<br>Attribution: %{y:.1f}%',\n",
        "                marker_color=px.colors.sequential.Blues[3]\n",
        "            ),\n",
        "            row=1, col=2\n",
        "        )\n",
        "\n",
        "        # 3. Conversion rates by channel (bottom-left)\n",
        "        # Use a simple set of dummy conversion rates if customer journey data not available\n",
        "        conversion_data = pd.DataFrame({\n",
        "            'touchpoint': comparison['touchpoint'].head(8),\n",
        "            'conversion_rate': np.random.uniform(0.02, 0.15, 8)\n",
        "        })\n",
        "\n",
        "        fig.add_trace(\n",
        "            go.Bar(\n",
        "                x=conversion_data['touchpoint'],\n",
        "                y=conversion_data['conversion_rate'],\n",
        "                marker_color=px.colors.sequential.Greens[4],\n",
        "                name='Conversion Rate'\n",
        "            ),\n",
        "            row=2, col=1\n",
        "        )\n",
        "\n",
        "        # 4. Attribution over time (bottom-right) - simulated data\n",
        "        # Create synthetic time series data for demonstration\n",
        "        dates = pd.date_range(start='2023-01-01', periods=12, freq='M')\n",
        "        time_models = ['first_touch', 'last_touch', 'linear']\n",
        "\n",
        "        for i, model in enumerate(time_models):\n",
        "            values = 20 + 5 * np.sin(np.linspace(0, 2*np.pi, 12) + i) + np.random.normal(0, 2, 12)\n",
        "            fig.add_trace(\n",
        "                go.Scatter(\n",
        "                    x=dates,\n",
        "                    y=values,\n",
        "                    mode='lines+markers',\n",
        "                    name=model,\n",
        "                    marker_color=px.colors.qualitative.Plotly[i]\n",
        "                ),\n",
        "                row=2, col=2\n",
        "            )\n",
        "\n",
        "        # Update layout\n",
        "        fig.update_layout(\n",
        "            title='Marketing Attribution Dashboard',\n",
        "            height=800,\n",
        "            showlegend=True,\n",
        "            legend=dict(orientation='h', y=-0.1)\n",
        "        )\n",
        "\n",
        "        # Update axes\n",
        "        fig.update_xaxes(title_text='Channel', row=1, col=1)\n",
        "        fig.update_yaxes(title_text='Attribution (%)', row=1, col=1)\n",
        "\n",
        "        fig.update_xaxes(title_text='Attribution Model', row=1, col=2)\n",
        "        fig.update_yaxes(title_text='Top Channel Attribution (%)', row=1, col=2)\n",
        "\n",
        "        fig.update_xaxes(title_text='Channel', row=2, col=1)\n",
        "        fig.update_yaxes(title_text='Conversion Rate', row=2, col=1, tickformat='.1%')\n",
        "\n",
        "        fig.update_xaxes(title_text='Month', row=2, col=2)\n",
        "        fig.update_yaxes(title_text='Attribution (%)', row=2, col=2)\n",
        "\n",
        "        if save_path:\n",
        "            fig.write_html(save_path)\n",
        "            logger.info(f\"Interactive dashboard saved to {save_path}\")\n",
        "\n",
        "        return fig\n",
        "\n",
        "\n",
        "class MarketingAttribution:\n",
        "    \"\"\"\n",
        "    Main class integrating all components of the marketing attribution system.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config=None):\n",
        "        \"\"\"\n",
        "        Initialize the marketing attribution system.\n",
        "\n",
        "        Args:\n",
        "            config: Configuration parameters for the system\n",
        "        \"\"\"\n",
        "        self.config = config or {}\n",
        "        self.data_ingestion = DataIngestion(config)\n",
        "        self.customer_journey = CustomerJourney()\n",
        "        self.attribution_models = AttributionModels()\n",
        "        self.advanced_analytics = AdvancedAnalytics()\n",
        "        self.visualization = Visualization()\n",
        "\n",
        "        logger.info(\"Marketing Attribution system initialized\")\n",
        "\n",
        "    def ingest_data(self, **kwargs):\n",
        "        \"\"\"\n",
        "        Ingest data from various sources.\n",
        "\n",
        "        Args:\n",
        "            **kwargs: Data source parameters as required by DataIngestion methods\n",
        "\n",
        "        Returns:\n",
        "            The unified customer data\n",
        "        \"\"\"\n",
        "        # This method should be implemented based on specific data ingestion needs\n",
        "        # For example:\n",
        "        # self.data_ingestion.ingest_csv('website', 'website_data.csv')\n",
        "        # self.data_ingestion.ingest_api_data('social', 'https://api.example.com/data')\n",
        "\n",
        "        # If unify_customer_data parameters are provided in kwargs\n",
        "        if 'id_mapping_columns' in kwargs:\n",
        "            unified_data = self.data_ingestion.unify_customer_data(\n",
        "                id_mapping_columns=kwargs['id_mapping_columns'],\n",
        "                timestamp_columns=kwargs.get('timestamp_columns'),\n",
        "                conversion_column=kwargs.get('conversion_column')\n",
        "            )\n",
        "        else:\n",
        "            unified_data = self.data_ingestion.unified_data\n",
        "\n",
        "        if unified_data is not None:\n",
        "            self.customer_journey.set_data(unified_data)\n",
        "\n",
        "        return unified_data\n",
        "\n",
        "    def create_customer_journeys(self, **kwargs):\n",
        "        \"\"\"\n",
        "        Create customer journeys from the unified data.\n",
        "\n",
        "        Args:\n",
        "            **kwargs: Parameters for CustomerJourney.create_customer_journeys\n",
        "\n",
        "        Returns:\n",
        "            The customer journeys\n",
        "        \"\"\"\n",
        "        journeys = self.customer_journey.create_customer_journeys(**kwargs)\n",
        "\n",
        "        if journeys is not None:\n",
        "            self.attribution_models.set_journeys(journeys)\n",
        "            self.advanced_analytics.set_data(journeys=journeys)\n",
        "\n",
        "        return journeys\n",
        "\n",
        "    def run_attribution_models(self, models=None):\n",
        "        \"\"\"\n",
        "        Run selected attribution models.\n",
        "\n",
        "        Args:\n",
        "            models: List of model names to run (None means all available models)\n",
        "\n",
        "        Returns:\n",
        "            Dict with attribution results\n",
        "        \"\"\"\n",
        "        if models is None:\n",
        "            models = [\n",
        "                'first_touch',\n",
        "                'last_touch',\n",
        "                'linear',\n",
        "                'position_based',\n",
        "                'time_decay',\n",
        "                'markov_chain',\n",
        "                'shapley',\n",
        "                'logistic_regression'\n",
        "            ]\n",
        "\n",
        "        results = {}\n",
        "\n",
        "        for model in models:\n",
        "            try:\n",
        "                if model == 'first_touch':\n",
        "                    result = self.attribution_models.run_first_touch_attribution()\n",
        "                elif model == 'last_touch':\n",
        "                    result = self.attribution_models.run_last_touch_attribution()\n",
        "                elif model == 'linear':\n",
        "                    result = self.attribution_models.run_linear_attribution()\n",
        "                elif model == 'position_based':\n",
        "                    result = self.attribution_models.run_position_based_attribution()\n",
        "                elif model == 'time_decay':\n",
        "                    result = self.attribution_models.run_time_decay_attribution()\n",
        "                elif model == 'markov_chain':\n",
        "                    result = self.attribution_models.run_markov_chain_attribution()\n",
        "                elif model == 'shapley':\n",
        "                    result = self.attribution_models.run_shapley_value_attribution(max_combination_size=3)\n",
        "                elif model == 'logistic_regression':\n",
        "                    result = self.attribution_models.run_logistic_regression_attribution()\n",
        "                elif model == 'deep_learning':\n",
        "                    result = self.attribution_models.run_deep_learning_attribution()\n",
        "                else:\n",
        "                    logger.warning(f\"Unknown attribution model: {model}\")\n",
        "                    continue\n",
        "\n",
        "                if result is not None:\n",
        "                    results[model] = result\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error running {model} attribution model: {str(e)}\")\n",
        "\n",
        "        self.visualization.set_components(attribution_models=self.attribution_models)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def run_advanced_analytics(self, analyses=None):\n",
        "        \"\"\"\n",
        "        Run selected advanced analytics.\n",
        "\n",
        "        Args:\n",
        "            analyses: List of analysis names to run (None means all available analyses)\n",
        "\n",
        "        Returns:\n",
        "            Dict with analytics results\n",
        "        \"\"\"\n",
        "        if analyses is None:\n",
        "            analyses = [\n",
        "                'network_analysis',\n",
        "                'customer_segmentation'\n",
        "            ]\n",
        "\n",
        "        results = {}\n",
        "\n",
        "        for analysis in analyses:\n",
        "            try:\n",
        "                if analysis == 'network_analysis':\n",
        "                    result = self.advanced_analytics.run_network_analysis()\n",
        "                elif analysis == 'customer_segmentation':\n",
        "                    result = self.advanced_analytics.customer_segmentation()\n",
        "                else:\n",
        "                    logger.warning(f\"Unknown analysis: {analysis}\")\n",
        "                    continue\n",
        "\n",
        "                if result is not None:\n",
        "                    results[analysis] = result\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error running {analysis}: {str(e)}\")\n",
        "\n",
        "        self.visualization.set_components(advanced_analytics=self.advanced_analytics)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def create_visualizations(self, visualizations=None, output_dir=None):\n",
        "        \"\"\"\n",
        "        Create selected visualizations.\n",
        "\n",
        "        Args:\n",
        "            visualizations: List of visualization names to create (None means all available visualizations)\n",
        "            output_dir: Directory to save visualizations\n",
        "\n",
        "        Returns:\n",
        "            Dict with visualization figures\n",
        "        \"\"\"\n",
        "        if visualizations is None:\n",
        "            visualizations = [\n",
        "                'attribution_comparison',\n",
        "                'customer_journey',\n",
        "                'network_graph',\n",
        "                'customer_segments',\n",
        "                'interactive_dashboard'\n",
        "            ]\n",
        "\n",
        "        figures = {}\n",
        "\n",
        "        for viz in visualizations:\n",
        "            try:\n",
        "                save_path = f\"{output_dir}/{viz}.html\" if output_dir else None\n",
        "\n",
        "                if viz == 'attribution_comparison':\n",
        "                    fig = self.visualization.plot_attribution_comparison(save_path=save_path)\n",
        "                elif viz == 'customer_journey':\n",
        "                    fig = self.visualization.plot_customer_journey_sankey(save_path=save_path)\n",
        "                elif viz == 'network_graph':\n",
        "                    fig = self.visualization.plot_network_graph(save_path=save_path)\n",
        "                elif viz == 'customer_segments':\n",
        "                    fig = self.visualization.plot_customer_segments(save_path=save_path)\n",
        "                elif viz == 'interactive_dashboard':\n",
        "                    fig = self.visualization.create_interactive_dashboard(save_path=save_path)\n",
        "                else:\n",
        "                    logger.warning(f\"Unknown visualization: {viz}\")\n",
        "                    continue\n",
        "\n",
        "                if fig is not None:\n",
        "                    figures[viz] = fig\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error creating {viz} visualization: {str(e)}\")\n",
        "\n",
        "        return figures\n",
        "\n",
        "    def generate_report(self, output_path=None):\n",
        "        \"\"\"\n",
        "        Generate a comprehensive report of attribution insights.\n",
        "\n",
        "        Args:\n",
        "            output_path: Path to save the report\n",
        "\n",
        "        Returns:\n",
        "            HTML report as a string\n",
        "        \"\"\"\n",
        "        if not self.attribution_models.results:\n",
        "            logger.warning(\"No attribution results available for report\")\n",
        "            return None\n",
        "\n",
        "        # Create report HTML\n",
        "        report_html = f\"\"\"\n",
        "        <!DOCTYPE html>\n",
        "        <html>\n",
        "        <head>\n",
        "            <title>Marketing Attribution Analysis Report</title>\n",
        "            <style>\n",
        "                body {{ font-family: Arial, sans-serif; margin: 20px; }}\n",
        "                h1, h2, h3 {{ color: #2c3e50; }}\n",
        "                .container {{ max-width: 1200px; margin: 0 auto; }}\n",
        "                .section {{ margin-bottom: 30px; padding: 20px; border: 1px solid #ddd; border-radius: 5px; }}\n",
        "                .metric {{ display: inline-block; width: 200px; text-align: center; margin: 10px;\n",
        "                          padding: 15px; background-color: #f8f9fa; border-radius: 5px; }}\n",
        "                .metric h3 {{ margin-top: 0; }}\n",
        "                .metric p {{ font-size: 24px; font-weight: bold; margin: 5px 0; }}\n",
        "                table {{ border-collapse: collapse; width: 100%; }}\n",
        "                th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}\n",
        "                th {{ background-color: #f2f2f2; }}\n",
        "                tr:nth-child(even) {{ background-color: #f9f9f9; }}\n",
        "            </style>\n",
        "        </head>\n",
        "        <body>\n",
        "            <div class=\"container\">\n",
        "                <h1>Marketing Attribution Analysis Report</h1>\n",
        "                <p>Generated on {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>\n",
        "\n",
        "                <div class=\"section\">\n",
        "                    <h2>Executive Summary</h2>\n",
        "                    <div class=\"metrics-container\">\n",
        "        \"\"\"\n",
        "\n",
        "        # Add key metrics\n",
        "        conversion_count = 0\n",
        "        if self.customer_journey and self.customer_journey.journeys:\n",
        "            conversion_count = sum(1 for journey in self.customer_journey.journeys.values() if journey.get('converted', False))\n",
        "\n",
        "        total_journeys = len(self.customer_journey.journeys) if self.customer_journey and self.customer_journey.journeys else 0\n",
        "        conversion_rate = (conversion_count / total_journeys * 100) if total_journeys > 0 else 0\n",
        "\n",
        "        report_html += f\"\"\"\n",
        "                        <div class=\"metric\">\n",
        "                            <h3>Total Journeys</h3>\n",
        "                            <p>{total_journeys}</p>\n",
        "                        </div>\n",
        "                        <div class=\"metric\">\n",
        "                            <h3>Conversions</h3>\n",
        "                            <p>{conversion_count}</p>\n",
        "                        </div>\n",
        "                        <div class=\"metric\">\n",
        "                            <h3>Conversion Rate</h3>\n",
        "                            <p>{conversion_rate:.1f}%</p>\n",
        "                        </div>\n",
        "        \"\"\"\n",
        "\n",
        "        # Add top performing channels\n",
        "        if self.attribution_models.results:\n",
        "            # Use results from the most advanced model available\n",
        "            preferred_models = ['shapley', 'markov_chain', 'deep_learning', 'logistic_regression',\n",
        "                               'time_decay', 'position_based', 'linear', 'last_touch', 'first_touch']\n",
        "\n",
        "            model_name = next((model for model in preferred_models if model in self.attribution_models.results), None)\n",
        "\n",
        "            if model_name:\n",
        "                top_channels = self.attribution_models.results[model_name].sort_values('attribution_percentage', ascending=False).head(3)\n",
        "\n",
        "                report_html += f\"\"\"\n",
        "                    </div>\n",
        "                    <h3>Top Performing Channels ({model_name.replace('_', ' ').title()})</h3>\n",
        "                    <table>\n",
        "                        <tr>\n",
        "                            <th>Channel</th>\n",
        "                            <th>Attribution %</th>\n",
        "                        </tr>\n",
        "                \"\"\"\n",
        "\n",
        "                for _, row in top_channels.iterrows():\n",
        "                    report_html += f\"\"\"\n",
        "                        <tr>\n",
        "                            <td>{row['touchpoint']}</td>\n",
        "                            <td>{row['attribution_percentage']:.1f}%</td>\n",
        "                        </tr>\n",
        "                    \"\"\"\n",
        "\n",
        "                report_html += \"</table>\"\n",
        "\n",
        "        # Add model comparison\n",
        "        report_html += \"\"\"\n",
        "                </div>\n",
        "\n",
        "                <div class=\"section\">\n",
        "                    <h2>Attribution Model Comparison</h2>\n",
        "        \"\"\"\n",
        "\n",
        "        if self.attribution_models.results:\n",
        "            comparison = self.attribution_models.compare_models()\n",
        "\n",
        "            if comparison is not None:\n",
        "                model_cols = [col for col in comparison.columns if col.endswith('_percentage')]\n",
        "\n",
        "                report_html += \"\"\"\n",
        "                    <table>\n",
        "                        <tr>\n",
        "                            <th>Touchpoint</th>\n",
        "                \"\"\"\n",
        "\n",
        "                for col in model_cols:\n",
        "                    model_name = col.replace('_percentage', '').replace('_', ' ').title()\n",
        "                    report_html += f\"<th>{model_name}</th>\"\n",
        "\n",
        "                report_html += \"</tr>\"\n",
        "\n",
        "                # Sort by average attribution\n",
        "                comparison['avg_attribution'] = comparison[model_cols].mean(axis=1)\n",
        "                comparison = comparison.sort_values('avg_attribution', ascending=False).head(10)\n",
        "\n",
        "                for _, row in comparison.iterrows():\n",
        "                    report_html += f\"<tr><td>{row['touchpoint']}</td>\"\n",
        "\n",
        "                    for col in model_cols:\n",
        "                        report_html += f\"<td>{row[col]:.1f}%</td>\"\n",
        "\n",
        "                    report_html += \"</tr>\"\n",
        "\n",
        "                report_html += \"</table>\"\n",
        "\n",
        "        # Add customer journey insights\n",
        "        report_html += \"\"\"\n",
        "                </div>\n",
        "\n",
        "                <div class=\"section\">\n",
        "                    <h2>Customer Journey Insights</h2>\n",
        "        \"\"\"\n",
        "\n",
        "        if self.customer_journey and self.customer_journey.touchpoint_sequence is not None:\n",
        "            top_journeys = self.customer_journey.get_most_common_journeys(top_n=5)\n",
        "\n",
        "            if top_journeys is not None:\n",
        "                report_html += \"\"\"\n",
        "                    <h3>Most Common Customer Journeys</h3>\n",
        "                    <table>\n",
        "                        <tr>\n",
        "                            <th>Journey</th>\n",
        "                            <th>Count</th>\n",
        "                            <th>Conversion Rate</th>\n",
        "                        </tr>\n",
        "                \"\"\"\n",
        "\n",
        "                for _, row in top_journeys.iterrows():\n",
        "                    report_html += f\"\"\"\n",
        "                        <tr>\n",
        "                            <td>{row['journey']}</td>\n",
        "                            <td>{row['count']}</td>\n",
        "                            <td>{row['conversion_rate']*100:.1f}%</td>\n",
        "                        </tr>\n",
        "                    \"\"\"\n",
        "\n",
        "                report_html += \"</table>\"\n",
        "\n",
        "        # Add advanced analytics insights\n",
        "        if self.advanced_analytics.results:\n",
        "            report_html += \"\"\"\n",
        "                </div>\n",
        "\n",
        "                <div class=\"section\">\n",
        "                    <h2>Advanced Analytics Insights</h2>\n",
        "            \"\"\"\n",
        "\n",
        "            # Add customer segmentation insights\n",
        "            if 'customer_segments' in self.advanced_analytics.results:\n",
        "                profiles = self.advanced_analytics.results['customer_segments']['profiles']\n",
        "\n",
        "                report_html += \"\"\"\n",
        "                    <h3>Customer Segments</h3>\n",
        "                    <table>\n",
        "                        <tr>\n",
        "                            <th>Segment</th>\n",
        "                            <th>Size</th>\n",
        "                            <th>Conversion Rate</th>\n",
        "                            <th>Top Touchpoints</th>\n",
        "                        </tr>\n",
        "                \"\"\"\n",
        "\n",
        "                for _, row in profiles.iterrows():\n",
        "                    top_touchpoints = ', '.join(row['top_touchpoints'])\n",
        "\n",
        "                    report_html += f\"\"\"\n",
        "                        <tr>\n",
        "                            <td>Cluster {int(row['cluster'])}</td>\n",
        "                            <td>{int(row['size'])}</td>\n",
        "                            <td>{row['conversion_rate']*100:.1f}%</td>\n",
        "                            <td>{top_touchpoints}</td>\n",
        "                        </tr>\n",
        "                    \"\"\"\n",
        "\n",
        "                report_html += \"</table>\"\n",
        "\n",
        "        # Close report HTML\n",
        "        report_html += \"\"\"\n",
        "                </div>\n",
        "            </div>\n",
        "        </body>\n",
        "        </html>\n",
        "        \"\"\"\n",
        "\n",
        "        # Save report if output path is provided\n",
        "        if output_path:\n",
        "            try:\n",
        "                with open(output_path, 'w') as f:\n",
        "                    f.write(report_html)\n",
        "                logger.info(f\"Report saved to {output_path}\")\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error saving report to {output_path}: {str(e)}\")\n",
        "\n",
        "        return report_html"
      ],
      "metadata": {
        "id": "ynJohc8V5LcA"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def example_usage():\n",
        "    \"\"\"Fixed example usage with visualization fixes.\"\"\"\n",
        "    import os\n",
        "    import pandas as pd\n",
        "\n",
        "    # Initialize the system\n",
        "    attribution_system = MarketingAttribution()\n",
        "\n",
        "    # 1. Data ingestion\n",
        "    print(\"Ingesting data...\")\n",
        "    attribution_system.data_ingestion.ingest_csv(\n",
        "        name='website_analytics',\n",
        "        file_path='simulated_data/website_analytics.csv'\n",
        "    )\n",
        "\n",
        "    attribution_system.data_ingestion.ingest_csv(\n",
        "        name='email_campaigns',\n",
        "        file_path='simulated_data/email_campaigns.csv'\n",
        "    )\n",
        "\n",
        "    attribution_system.data_ingestion.ingest_csv(\n",
        "        name='paid_ads',\n",
        "        file_path='simulated_data/paid_ads.csv'\n",
        "    )\n",
        "\n",
        "    # 2. Unify customer data\n",
        "    print(\"Unifying customer data...\")\n",
        "    unified_data = attribution_system.data_ingestion.unify_customer_data(\n",
        "        id_mapping_columns={\n",
        "            'website_analytics': 'user_id',\n",
        "            'email_campaigns': 'email_id',\n",
        "            'paid_ads': 'click_id'\n",
        "        },\n",
        "        timestamp_columns={\n",
        "            'website_analytics': 'timestamp',\n",
        "            'email_campaigns': 'date',\n",
        "            'paid_ads': 'click_time'\n",
        "        },\n",
        "        conversion_column='purchase'\n",
        "    )\n",
        "\n",
        "    # Use pre-generated unified data if needed\n",
        "    if unified_data is None:\n",
        "        print(\"Using pre-generated unified data...\")\n",
        "        try:\n",
        "            # Load pre-generated unified data\n",
        "            pre_unified = pd.read_csv('simulated_data/unified_data.csv')\n",
        "            pre_unified['timestamp'] = pd.to_datetime(pre_unified['timestamp'])\n",
        "            pre_unified['is_conversion'] = pre_unified['is_conversion'].astype(bool)\n",
        "\n",
        "            # Set the data directly\n",
        "            attribution_system.customer_journey.set_data(pre_unified)\n",
        "            unified_data = pre_unified\n",
        "            print(f\"Successfully set pre-unified data with {len(pre_unified)} records\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading pre-unified data: {str(e)}\")\n",
        "            return\n",
        "\n",
        "    # 3. Create customer journeys\n",
        "    print(\"Creating customer journeys...\")\n",
        "    journeys = attribution_system.customer_journey.create_customer_journeys(\n",
        "        customer_id_col='customer_id',\n",
        "        timestamp_col='timestamp',\n",
        "        channel_col='data_source',\n",
        "        conversion_col='is_conversion'\n",
        "    )\n",
        "\n",
        "    # 4. If the system's method fails, use manual journey creation\n",
        "    if not journeys:\n",
        "        print(\"Using manual journey creation...\")\n",
        "        journeys = manual_customer_journey_creation(unified_data)\n",
        "\n",
        "        # Apply visualization fixes\n",
        "        fix_visualization_issues(attribution_system, journeys)\n",
        "\n",
        "    # 5. Add funnel stages\n",
        "    attribution_system.customer_journey.add_funnel_stages({\n",
        "        'website_analytics': 'awareness',\n",
        "        'email_campaigns': 'consideration',\n",
        "        'paid_ads': 'conversion'\n",
        "    })\n",
        "\n",
        "    # 6. Run attribution models\n",
        "    print(\"Running attribution models...\")\n",
        "    try:\n",
        "        attribution_results = attribution_system.run_attribution_models([\n",
        "            'first_touch',\n",
        "            'last_touch',\n",
        "            'linear'\n",
        "        ])\n",
        "        print(\"Attribution models completed successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error running attribution models: {str(e)}\")\n",
        "\n",
        "    # 7. Create visualizations\n",
        "    print(\"Creating visualizations...\")\n",
        "    output_dir = 'attribution_results'\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        # Fix the visualization format issue - Use .png instead of .html for matplotlib figures\n",
        "        visualizations = attribution_system.create_visualizations(\n",
        "            visualizations=[\n",
        "                'customer_journey'  # Use only plotly-based visualizations first\n",
        "            ],\n",
        "            output_dir=output_dir\n",
        "        )\n",
        "        print(\"Customer journey visualization created successfully\")\n",
        "\n",
        "        # Try matplotlib-based visualizations with correct format\n",
        "        try:\n",
        "            # Create a separate directory for matplotlib figures\n",
        "            matplotlib_dir = os.path.join(output_dir, 'matplotlib')\n",
        "            os.makedirs(matplotlib_dir, exist_ok=True)\n",
        "\n",
        "            # Try attribution comparison with PNG format\n",
        "            save_path = os.path.join(matplotlib_dir, 'attribution_comparison.png')\n",
        "            fig = attribution_system.visualization.plot_attribution_comparison(save_path=save_path)\n",
        "            print(\"Attribution comparison visualization created successfully\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error creating matplotlib visualization: {str(e)}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating visualizations: {str(e)}\")\n",
        "\n",
        "    # 8. Generate report\n",
        "    try:\n",
        "        report = attribution_system.generate_report(\n",
        "            output_path=f\"{output_dir}/attribution_report.html\"\n",
        "        )\n",
        "        print(\"Report generated successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating report: {str(e)}\")\n",
        "\n",
        "    print(f\"Marketing attribution analysis completed. Results saved to {output_dir}\")\n",
        "    return attribution_system\n",
        "if __name__ == \"__main__\":\n",
        "    example_usage()"
      ],
      "metadata": {
        "id": "fcwnyjq9ixUP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d7ce108b-c09d-47a0-a248-ff7623606aac"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:marketing_attribution:No data available for creating customer journeys\n",
            "WARNING:marketing_attribution:No data or journeys available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ingesting data...\n",
            "Unifying customer data...\n",
            "Creating customer journeys...\n",
            "Using manual journey creation...\n",
            "\n",
            "Manually creating customer journeys...\n",
            "Manually created 500 customer journeys\n",
            "\n",
            "Sample journeys (first 5):\n",
            "\n",
            "Journey 1:\n",
            "  Customer: cust_0000\n",
            "  Touchpoints: ['paid_ads', 'paid_ads', 'website_analytics']\n",
            "  Converted: False\n",
            "\n",
            "Journey 2:\n",
            "  Customer: cust_0001\n",
            "  Touchpoints: ['email_campaigns', 'email_campaigns', 'paid_ads', 'paid_ads', 'website_analytics', 'website_analytics', 'email_campaigns']\n",
            "  Converted: False\n",
            "\n",
            "Journey 3:\n",
            "  Customer: cust_0002\n",
            "  Touchpoints: ['paid_ads', 'paid_ads', 'email_campaigns', 'paid_ads']\n",
            "  Converted: False\n",
            "\n",
            "Journey 4:\n",
            "  Customer: cust_0003\n",
            "  Touchpoints: ['website_analytics', 'email_campaigns', 'email_campaigns', 'email_campaigns', 'email_campaigns', 'email_campaigns']\n",
            "  Converted: False\n",
            "\n",
            "Journey 5:\n",
            "  Customer: cust_0004\n",
            "  Touchpoints: ['paid_ads', 'email_campaigns', 'email_campaigns', 'paid_ads', 'paid_ads', 'paid_ads', 'paid_ads']\n",
            "  Converted: False\n",
            "\n",
            "--- Applying visualization fixes ---\n",
            "Setting journeys in customer_journey component...\n",
            "Creating touchpoint_sequence...\n",
            "Setting touchpoint_sequence in customer_journey component...\n",
            "Creating conversion_rates...\n",
            "Setting journeys in attribution_models component...\n",
            "Setting data in advanced_analytics component...\n",
            "Setting components in visualization component...\n",
            "Visualization fixes applied successfully!\n",
            "Running attribution models...\n",
            "Attribution models completed successfully\n",
            "Creating visualizations...\n",
            "Customer journey visualization created successfully\n",
            "Attribution comparison visualization created successfully\n",
            "Report generated successfully\n",
            "Marketing attribution analysis completed. Results saved to attribution_results\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}